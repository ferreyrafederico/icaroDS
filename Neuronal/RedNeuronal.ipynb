{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neuronales Teoría."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nomenclatura\n",
    "$X$ Matriz de entrada, de ***Registros-Variables Predictivas*** de dimensiones ($m$,$n_0$)  \n",
    "$m$ es el número de ejemplos registros o muestras, de entrenamiento.  \n",
    "$n_0$ es el número de variables predictivas que coincide con el número de neuronas de la capa cero.  \n",
    "$Y$ Matriz de salida de los \"$y_p$\" valores predichos de dimensiones ($m$,$n_L$)  \n",
    "$n_L$ es el número de clases posibles para los valores predichos que coincide con el número de neuronas de la última capa \"$L$\".  \n",
    "$\\sigma_l$ Función de activación de la capa $l$.  \n",
    "$Z_l$ Matriz de entradas ponderadas de la capa $l$ antes de aplicar la función de activación $\\sigma_l$. $Z_l$ de dimensiones ($m$,$n_l$).  \n",
    "$n_l$ es el número de neuronas de la capa $l$  \n",
    "$A_l$ Matriz de activaciones de entrada de la capa $l$, de dimensiones ($m$,$n_l$).  \n",
    "$W_l$ Matriz de pesos de la capa $l$, de dimensiones ($n_{l-1}$,$n_l$).  \n",
    "$b_l$ Vector de sesgos de la capa $l$, de dimensiones ($1$,$n_l$)  \n",
    "$\\alpha$ Tasa de \"aprendizaje\".  \n",
    "Se utiliza para regular la magnitud del cambio en los pesos en dirección opuesta al gradiente.  \n",
    "Una tasa de aprendizaje alta puede llevar a pasos demasiado grandes que pueden hacer que el algoritmo de optimización oscile o diverja, mientras que una tasa de aprendizaje baja puede hacer que el entrenamiento sea demasiado lento o quede atrapado en óptimos locales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proceso de Propagación.\n",
    "1. Se Inicializa $A_0=X$ , es decir $A=[X]$      \n",
    "   Dimensiones:  \n",
    "     \n",
    "   $A_0 (m,n_0)$  \n",
    "     \n",
    "   $X=Z_0$  $(m,n_0)$    \n",
    "   \n",
    "   $A (1,L)$ $A$ es un arreglo de $L$ matrices de $(m,n_i)$  \n",
    "\n",
    "2. Para $l=1,2,...,L-1$ se calcula $Z_l=A_{l-1}W_l+b_l$ y luego $A_l=\\sigma_l(Z_l)$  \n",
    "   Dimensiones:  \n",
    "     \n",
    "   $A_l=\\sigma_l(Z_l)$ $(m,n_{l-1})$ $(n_{l-1},n_l)$ + $(1,b_l)$ $\\implies (m,n_l)$\n",
    "     \n",
    "3. Finalmente se calcula $A_L=\\sigma_L(Z_L)=y_p$ de modo que\n",
    "4. $A=[X,A_1,A_2,...,A_{L-1},y_p]$ es un arreglo de $L$ matrices de $(m,n_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proceso de Retropropagación.\n",
    "Se calcula el gradiente de la función de costo $J(W,b)$ con respecto a los pesos $W_l$ y los sesgos $b_l$ de cada capa:\n",
    "$$\\frac{\\partial J}{\\partial{W_l}}=\\frac{\\partial J}{\\partial{Z_l}}\\frac{\\partial Z_l}{\\partial{W_l}}=\\frac{\\partial J}{\\partial{A_l}}\\frac{\\partial A_l}{\\partial{Z_l}}\\frac{\\partial Z_l}{\\partial{W_l}}=\\frac{\\partial J}{\\partial{A_l}}\\frac{\\partial \\sigma_l(Z_l)}{\\partial{Z_l}}A_{l-1}^T$$ \n",
    "$$(n_{l-1},n_l)\\equiv(n_l,n_l)(n_l,m)$$\n",
    "$$\\frac{\\partial J}{\\partial{b_l}}=\\frac{\\partial J}{\\partial{Z_l}}\\frac{\\partial Z_l}{\\partial{b_l}}=\\frac{\\partial J}{\\partial{A_l}}\\frac{\\partial \\sigma_l(Z_l)}{\\partial{Z_l}}$$\n",
    "Entonces:  \n",
    "$$\\frac{\\partial J}{\\partial{W_l}}=\\frac{\\partial J}{\\partial{A_l}}\\dot{\\sigma}_l(Z_l)A_{l-1}^T$$\n",
    "$$\\frac{\\partial J}{\\partial{b_l}}=\\frac{\\partial J}{\\partial{A_l}}\\dot{\\sigma}_l(Z_l)$$\n",
    "Ahora se debe calcular:\n",
    "$$\\frac{\\partial J}{\\partial{A_l}}=\\frac{\\partial J}{\\partial{A_{l+1}}}\\frac{\\partial A_{l+1}}{\\partial{A_l}}=\\frac{\\partial J}{\\partial{A_{l+1}}}\\frac{\\partial \\sigma_{l+1}(Z_{l+1})}{\\partial{Z_{l+1}}}\\frac{\\partial Z_{l+1}}{\\partial A_l}=\\frac{\\partial J}{\\partial{A_{l+1}}}\\dot{\\sigma}_{l+1}(Z_{l+1})W^T_{l+1}$$\n",
    "De este modo se puede calcular recursivamente a partir de la salida o respuesta de la red:  \n",
    "$$\\frac{\\partial J}{\\partial{A_l}}=\\frac{\\partial J}{\\partial{A_{l+1}}}\\dot{\\sigma}_{l+1}(Z_{l+1})W^T_{l+1}$$\n",
    "Una vez calculado el gradiente de la función de costo con respecto a los parámetros, se actualizan los parámetros utilizando el método del gradiente descendente:  \n",
    "$$W_l=W_l-\\alpha\\frac{\\partial J}{\\partial{W_l}}$$\n",
    "$$b_l=b_l-\\alpha\\frac{\\partial J}{\\partial{b_l}}$$\n",
    "$$W_l=W_l-\\alpha\\frac{\\partial J}{\\partial{A_l}}\\dot{\\sigma}_l(Z_l)A_{l-1}^T$$\n",
    "$$b_l=b_l-\\alpha\\frac{\\partial J}{\\partial{A_l}}\\dot{\\sigma}_l(Z_l)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso particular cuando la funcion de costo es MSE.\n",
    "$$J(W,b)=MSE=\\frac{1}{m}(y_p-y)(y_p-y)^T=\\frac{1}{m}\\displaystyle\\sum_{i=1}^m(y_{pi}-y_i)^2$$\n",
    "además recordar:  \n",
    "$$y_p=A_L$$\n",
    "por lo tanto:\n",
    "$$J(W,b)=MSE=\\frac{1}{m}(A_L-y)(A_L-y)^T=\\frac{1}{m}\\displaystyle\\sum_{i=1}^m(A_{Li}-y_i)^2$$\n",
    "Entonces:  \n",
    "$$\\frac{\\partial J}{\\partial{A_L}}=\\frac{2}{m}(A_L-y)=\\frac{2}{m}\\delta_L$$\n",
    "Ahora para $l=L-1$:  \n",
    "$$\\frac{\\partial J}{\\partial{A_{L-1}}}=\\frac{2}{m}(A_L-y)=\\frac{2}{m}\\delta_L$$\n",
    "Finalmente para esta función de costo:  \n",
    "$$W_l=W_l-\\frac{\\alpha}{m}\\delta_l\\frac{\\partial \\sigma_l(Z_l)}{\\partial{Z_l}}A_{l-1}^T$$\n",
    "$$b_l=b_l-\\frac{\\alpha}{m}\\delta_l\\frac{\\partial \\sigma_l(Z_l)}{\\partial{Z_l}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de Activación y derivada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact(z, op,**kwargs):\n",
    "    funciones_activacion = {\n",
    "        'elu': elu,\n",
    "        'leaky_relu': leaky_relu,\n",
    "        'relu': relu,\n",
    "        'sigmoid': sigmoid,\n",
    "        'softplus': softplus,\n",
    "        'softmax': softmax,\n",
    "        'swish': swish,\n",
    "        'tanh': tanh\n",
    "    }\n",
    "    assert op in funciones_activacion, f\"Función de activación no válida. Por favor, elija entre {list(funciones_activacion.keys())}.\"\n",
    "    return funciones_activacion[op](z,**kwargs)\n",
    "\n",
    "def elu(z, dv=False, alpha=1.0):\n",
    "    if dv:\n",
    "        return np.where(z > 0, 1, alpha * np.exp(z))\n",
    "    else:\n",
    "        return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
    "\n",
    "def leaky_relu(z, dv=False, alpha=0.01):\n",
    "    if dv:\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "    else:\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "def relu(z, dv=False):\n",
    "    if dv:\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    else:\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "def sigmoid(z, dv=False):\n",
    "    sig = 1 / (1 + np.exp(-z))\n",
    "    if dv:\n",
    "        return sig * (1 - sig)\n",
    "    else:\n",
    "        return sig\n",
    "\n",
    "def softplus(z, dv=False):\n",
    "    if dv:\n",
    "        return sigmoid(z)\n",
    "    else:\n",
    "        return np.log(1 + np.exp(z))\n",
    "\n",
    "def softmax(z, dv=False):\n",
    "    exp_z = np.exp(z - np.max(z))\n",
    "    sm = exp_z / np.sum(exp_z)\n",
    "    if dv:\n",
    "        return np.diag(sm) - np.outer(sm, sm)\n",
    "    else:\n",
    "        return sm\n",
    "\n",
    "def swish(z, dv=False):\n",
    "    if dv:\n",
    "        sig = sigmoid(z)\n",
    "        return sig + z * sig * (1 - sig)\n",
    "    else:\n",
    "        return z * sigmoid(z)\n",
    "\n",
    "def tanh(z, dv=False):\n",
    "    if dv:\n",
    "        return 1 - np.tanh(z)**2\n",
    "    else:\n",
    "        return np.tanh(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, top=[], fact=relu,**kwargs):\n",
    "        self.top = top\n",
    "        self.fact = fact\n",
    "        self.fact_kwargs = kwargs\n",
    "        self.init_param()\n",
    "\n",
    "    def init_param(self):\n",
    "        self.ws = []\n",
    "        self.bs = []\n",
    "\n",
    "        # Inicializar pesos y sesgos para las capas ocultas\n",
    "        for i in range(1, len(self.top)):\n",
    "            w = np.random.randn(self.top[i], self.top[i-1])\n",
    "            b = np.zeros((self.top[i], 1))\n",
    "            self.ws.append(w)\n",
    "            self.bs.append(b)\n",
    "\n",
    "    def fwprop(self, X):\n",
    "        # Almacenar las activaciones de cada capa\n",
    "        activations = [X]\n",
    "        a = X\n",
    "\n",
    "        # Propagación hacia adelante\n",
    "        for w, b in zip(self.ws, self.bs):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = self.fact(z,**self.fact_kwargs)\n",
    "            activations.append(a)\n",
    "\n",
    "        return a, activations\n",
    "\n",
    "    def bprop(self, X, y, rate):\n",
    "        m = X.shape[1]\n",
    "        a, activations = self.fwprop(X)\n",
    "        deltas = deque([])\n",
    "\n",
    "        # Calcular el delta de la última capa\n",
    "        delta = a - y\n",
    "        deltas.appendleft(delta)\n",
    "\n",
    "        # Calcular los deltas de las capas ocultas\n",
    "        for l in range(len(self.ws) - 1, 0, -1):\n",
    "            delta = np.dot(self.ws[l].T, deltas[0]) * self.fact(activations[l],dv=1,**self.fact_kwargs)\n",
    "            deltas.appendleft(delta)\n",
    "\n",
    "        # Calcular los gradientes de los pesos y sesgos\n",
    "        dWs = [np.dot(d, activations[l].T) / m for l, d in enumerate(deltas)]\n",
    "        dbs = [np.sum(d, axis=1, keepdims=True) / m for d in deltas]\n",
    "\n",
    "        # Actualizar los pesos y sesgos\n",
    "        self.ws = [w - rate * dW for w, dW in zip(self.ws, dWs)]\n",
    "        self.bs = [b - rate * db for b, db in zip(self.bs, dbs)]\n",
    "        \n",
    "    def fit(self, X, y, rate=0.01, epochs=100):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.31551223,  0.50191654],\n",
       "        [-0.14717021,  0.51081106],\n",
       "        [ 0.13647864, -2.69844324],\n",
       "        [-1.16602382, -0.42805743],\n",
       "        [ 0.18587943, -0.47114309]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top=[2,5,1]\n",
    "w=np.random.randn(top[1], top[0])\n",
    "b = np.zeros((top[1], 1))\n",
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1],\n",
       "       [-4],\n",
       "       [10]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[array([[-1],\n",
       "        [-4],\n",
       "        [10]])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deltas=[]\n",
    "a=np.array([[1],[3],[5]])\n",
    "y=np.array([[2],[7],[-5]])\n",
    "delta = a - y\n",
    "# deltas.insert(0, delta)\n",
    "deltas.append(delta)\n",
    "display(delta)\n",
    "display(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "f3() missing 1 required positional argument: 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     res\u001b[38;5;241m=\u001b[39mm\u001b[38;5;241m+\u001b[39mf(n,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(f1(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[43mf3\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(f1(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,f4,d\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: f3() missing 1 required positional argument: 'n'"
     ]
    }
   ],
   "source": [
    "\n",
    "def f2(n):\n",
    "    return 2*n\n",
    "def f3(n,s=0):\n",
    "    return 2*n+s\n",
    "def f4(n,d=0,c=0):\n",
    "    return 2*n+d+c\n",
    "\n",
    "def f1(n,m,f,**kwargs):\n",
    "    res=m+f(n,**kwargs)\n",
    "    return res\n",
    "\n",
    "print(f1(1,0,f3(s=1)))\n",
    "print(f1(1,0,f4,d=1,c=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70544773, -1.23583869,  0.13192467,  1.52150908,  1.93326865]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=np.random.randn(1, 5)\n",
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
