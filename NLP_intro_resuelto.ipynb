{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX1iweEAKw8u"
      },
      "source": [
        "# NLP\n",
        "\n",
        "En esta clase aprendimos los siguientes conceptos:\n",
        "\n",
        "- Stop words\n",
        "- Palabra raiz (Lemma)\n",
        "- Tokenizado\n",
        "- Vectorizado (BOW / TFIDF)\n",
        "\n",
        "Ahora veremos como hacer esto en python.\n",
        "\n",
        "Para NLP introduciremos algunas librerías nuevas, una de ellas es [Spacy](https://spacy.io/).\n",
        "\n",
        "Otras librerías conocidas son:\n",
        "- nltk\n",
        "- gensim\n",
        "\n",
        "Y una librería que es de lo mejor que hay en NLP actualmente: Hugging face.\n",
        "\n",
        "Comenzaremos esta clase con Spacy. Ya viene pre-instalada en google colab por lo que no será necesario instalarla. Si luego trabajan en algún entorno en el que no este instalada, pueden seguir el tutorial de la página oficial.\n",
        "\n",
        "Para usar spacy, debemos descargar un modelo del lenguaje que vayamos a trabajar. En este caso estaremos trabajando con textos en inglés (ya está descargado en colab), pero si en otro momento utilizan otro idioma, deben descargarlo desde https://spacy.io/models.\n",
        "\n",
        "Ahora, importemos spacy y carguemos el modelo en inglés que utilizaremos en este notebook:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dSjRDc4LcBi"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p3s7NhuSlLY"
      },
      "source": [
        "### Carga de datos\n",
        "\n",
        "El siguiente comando descargará un dataset de reviews de películas (en inglés) en su entorno de colab.\n",
        "\n",
        "Luego de correr la siguiente celda, deberían ver en su entorno el directorio \"acllmdb\" que dentro contiene los datos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu_QI_F7LvoD",
        "outputId": "6f26ac4e-abe1-41b5-f3de-3eabb09a924a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tar (child): Cannot connect to C: resolve failed\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Child returned status 128\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ],
      "source": [
        "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "\n",
        "!tar -xvzf  C:\\Users\\apacek\\OneDrive - Practia\\Escritorio\\curso\\clases\\sprint 2\\Clase 28\\aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMSKeyLCbdvl"
      },
      "source": [
        "Si navegan el directorio que tiene los datos, veran que hay un directorio train y otro test.\n",
        "\n",
        "A su vez, dentro de cada uno de ellos podrán ver los directorios neg y pos. Ahi se encuentran los datos que utilizaremos hoy. \n",
        "\n",
        "En neg se encuentran reviews negativas, en pos review positivas.\n",
        "\n",
        "La siguiente celda, lista los nombres de los archivos que hay en /content/aclImdb/test/pos.\n",
        "\n",
        "Ven algo raro?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d6I02VocSOY",
        "outputId": "783952d6-dc72-45a1-d62b-d18f3d506180"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ls: cannot access '/aclImdb/test/pos/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls /aclImdb/test/pos/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK4HDzYRcdkB"
      },
      "source": [
        "Podemos ver que cada review está en un archivo .txt distinto. ¿Cómo podemos leer este tipo de archivos en pandas? Hasta ahora veníamos levantando únicamente CSVs.\n",
        "\n",
        "En python, se pueden abrir achivos con el comando:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "with open(\"ruta_al_archivo\", modo_de_lectura) as f:\n",
        "  # Acá ya tenemos acceso al archivo con el nombre f\n",
        "```\n",
        "\n",
        "donde modo_de_lectura puede ser:\n",
        "\n",
        "- r: read\n",
        "- w: write\n",
        "- a: append\n",
        "\n",
        "Más detalles: https://www.w3schools.com/python/ref_func_open.asp\n",
        "\n",
        "Además, si importamos el paquete \n",
        "\n",
        "```\n",
        "import os\n",
        "```\n",
        "\n",
        "podremos utilizar una función para listar el nomrbe de todos los archivos que se encuentran en un directorio:\n",
        "\n",
        "```\n",
        "os.listdir(\"directorio\")\n",
        "```\n",
        "\n",
        "Entonces, lo que vamos a hacer es abrir el directorio donde se encuentran los archivos y leerlos uno por uno. A todo esto lo guardaremos luego en un dataframe de pandas.\n",
        "\n",
        "Ejemplo de listado de reviews negativas con os.listdir:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ltAV5Od5zM",
        "outputId": "e3bd575a-6c38-4b5c-ffc3-bceae6a9c549"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['6243_1.txt',\n",
              " '5471_3.txt',\n",
              " '1534_1.txt',\n",
              " '1744_3.txt',\n",
              " '4736_2.txt',\n",
              " '9463_1.txt',\n",
              " '9827_1.txt',\n",
              " '7416_3.txt',\n",
              " '4163_3.txt',\n",
              " '8073_1.txt',\n",
              " '5241_3.txt',\n",
              " '9383_3.txt',\n",
              " '11505_3.txt',\n",
              " '10453_1.txt',\n",
              " '8710_1.txt',\n",
              " '4210_2.txt',\n",
              " '453_3.txt',\n",
              " '8662_1.txt',\n",
              " '3875_1.txt',\n",
              " '5880_1.txt',\n",
              " '4922_4.txt',\n",
              " '3670_3.txt',\n",
              " '4505_2.txt',\n",
              " '6450_1.txt',\n",
              " '7768_1.txt',\n",
              " '10811_1.txt',\n",
              " '7126_1.txt',\n",
              " '8633_4.txt',\n",
              " '10549_2.txt',\n",
              " '11654_4.txt',\n",
              " '6241_3.txt',\n",
              " '7564_1.txt',\n",
              " '9166_1.txt',\n",
              " '249_3.txt',\n",
              " '10063_1.txt',\n",
              " '10479_2.txt',\n",
              " '1037_1.txt',\n",
              " '7339_3.txt',\n",
              " '2167_2.txt',\n",
              " '7573_2.txt',\n",
              " '12259_1.txt',\n",
              " '11351_1.txt',\n",
              " '536_4.txt',\n",
              " '11166_1.txt',\n",
              " '8758_4.txt',\n",
              " '3501_1.txt',\n",
              " '9460_1.txt',\n",
              " '8498_3.txt',\n",
              " '6887_3.txt',\n",
              " '4872_4.txt',\n",
              " '5099_1.txt',\n",
              " '2827_3.txt',\n",
              " '4156_1.txt',\n",
              " '9707_3.txt',\n",
              " '2736_4.txt',\n",
              " '12139_2.txt',\n",
              " '8390_3.txt',\n",
              " '2794_3.txt',\n",
              " '58_3.txt',\n",
              " '5486_3.txt',\n",
              " '3180_2.txt',\n",
              " '11481_1.txt',\n",
              " '2595_3.txt',\n",
              " '4089_1.txt',\n",
              " '506_2.txt',\n",
              " '1904_1.txt',\n",
              " '8784_3.txt',\n",
              " '6074_3.txt',\n",
              " '10748_4.txt',\n",
              " '7452_4.txt',\n",
              " '4933_3.txt',\n",
              " '1973_1.txt',\n",
              " '11874_1.txt',\n",
              " '3490_3.txt',\n",
              " '3906_1.txt',\n",
              " '6874_4.txt',\n",
              " '3136_2.txt',\n",
              " '11145_1.txt',\n",
              " '10279_3.txt',\n",
              " '4021_1.txt',\n",
              " '9089_2.txt',\n",
              " '8550_3.txt',\n",
              " '7384_4.txt',\n",
              " '9743_1.txt',\n",
              " '1303_2.txt',\n",
              " '4508_2.txt',\n",
              " '2161_4.txt',\n",
              " '11480_2.txt',\n",
              " '3071_4.txt',\n",
              " '4723_1.txt',\n",
              " '2195_2.txt',\n",
              " '6811_4.txt',\n",
              " '10883_1.txt',\n",
              " '11916_3.txt',\n",
              " '4000_4.txt',\n",
              " '10028_2.txt',\n",
              " '4219_4.txt',\n",
              " '2730_3.txt',\n",
              " '6953_2.txt',\n",
              " '6590_3.txt',\n",
              " '12103_3.txt',\n",
              " '5003_4.txt',\n",
              " '9325_1.txt',\n",
              " '2656_1.txt',\n",
              " '9449_3.txt',\n",
              " '1105_2.txt',\n",
              " '2321_1.txt',\n",
              " '3394_4.txt',\n",
              " '10788_2.txt',\n",
              " '3495_1.txt',\n",
              " '6741_1.txt',\n",
              " '11952_1.txt',\n",
              " '7929_4.txt',\n",
              " '1768_4.txt',\n",
              " '4500_4.txt',\n",
              " '8209_4.txt',\n",
              " '8110_1.txt',\n",
              " '2113_3.txt',\n",
              " '6293_4.txt',\n",
              " '5605_1.txt',\n",
              " '7501_2.txt',\n",
              " '6798_1.txt',\n",
              " '3382_1.txt',\n",
              " '2448_4.txt',\n",
              " '7486_4.txt',\n",
              " '12179_2.txt',\n",
              " '12026_1.txt',\n",
              " '5592_2.txt',\n",
              " '11772_1.txt',\n",
              " '4849_1.txt',\n",
              " '7345_1.txt',\n",
              " '10687_1.txt',\n",
              " '11097_2.txt',\n",
              " '11196_1.txt',\n",
              " '2486_3.txt',\n",
              " '7819_1.txt',\n",
              " '1892_3.txt',\n",
              " '6478_1.txt',\n",
              " '6702_1.txt',\n",
              " '8520_1.txt',\n",
              " '8594_4.txt',\n",
              " '4643_4.txt',\n",
              " '7718_4.txt',\n",
              " '8777_2.txt',\n",
              " '11606_4.txt',\n",
              " '11793_3.txt',\n",
              " '10287_4.txt',\n",
              " '1014_2.txt',\n",
              " '1402_4.txt',\n",
              " '10313_4.txt',\n",
              " '3699_3.txt',\n",
              " '6587_4.txt',\n",
              " '1136_3.txt',\n",
              " '9471_1.txt',\n",
              " '10759_4.txt',\n",
              " '2020_2.txt',\n",
              " '10091_1.txt',\n",
              " '2395_4.txt',\n",
              " '1775_4.txt',\n",
              " '8101_1.txt',\n",
              " '5479_3.txt',\n",
              " '8483_1.txt',\n",
              " '96_1.txt',\n",
              " '10012_1.txt',\n",
              " '9565_3.txt',\n",
              " '1392_2.txt',\n",
              " '11370_1.txt',\n",
              " '1506_1.txt',\n",
              " '4820_3.txt',\n",
              " '6035_1.txt',\n",
              " '3334_4.txt',\n",
              " '10961_2.txt',\n",
              " '9527_3.txt',\n",
              " '10760_1.txt',\n",
              " '1427_2.txt',\n",
              " '6691_1.txt',\n",
              " '4100_3.txt',\n",
              " '5459_2.txt',\n",
              " '6310_3.txt',\n",
              " '5910_2.txt',\n",
              " '320_1.txt',\n",
              " '5426_2.txt',\n",
              " '294_4.txt',\n",
              " '2150_4.txt',\n",
              " '8826_2.txt',\n",
              " '10182_3.txt',\n",
              " '7349_1.txt',\n",
              " '11903_2.txt',\n",
              " '30_1.txt',\n",
              " '1429_1.txt',\n",
              " '4857_2.txt',\n",
              " '2603_1.txt',\n",
              " '4592_1.txt',\n",
              " '8426_4.txt',\n",
              " '3568_4.txt',\n",
              " '10954_1.txt',\n",
              " '1963_4.txt',\n",
              " '1086_3.txt',\n",
              " '5504_1.txt',\n",
              " '4638_3.txt',\n",
              " '2651_4.txt',\n",
              " '11466_2.txt',\n",
              " '7264_1.txt',\n",
              " '8893_4.txt',\n",
              " '5845_3.txt',\n",
              " '4047_1.txt',\n",
              " '10556_1.txt',\n",
              " '3027_1.txt',\n",
              " '5074_2.txt',\n",
              " '12451_2.txt',\n",
              " '1220_4.txt',\n",
              " '11128_2.txt',\n",
              " '11844_3.txt',\n",
              " '11041_4.txt',\n",
              " '1469_1.txt',\n",
              " '10300_3.txt',\n",
              " '10766_4.txt',\n",
              " '223_1.txt',\n",
              " '11515_1.txt',\n",
              " '505_2.txt',\n",
              " '1584_1.txt',\n",
              " '10004_3.txt',\n",
              " '12437_1.txt',\n",
              " '1728_3.txt',\n",
              " '3540_2.txt',\n",
              " '7037_1.txt',\n",
              " '3753_1.txt',\n",
              " '288_1.txt',\n",
              " '8351_4.txt',\n",
              " '1291_4.txt',\n",
              " '10061_4.txt',\n",
              " '11229_1.txt',\n",
              " '10528_3.txt',\n",
              " '11029_4.txt',\n",
              " '4147_1.txt',\n",
              " '850_1.txt',\n",
              " '11699_1.txt',\n",
              " '644_2.txt',\n",
              " '12368_3.txt',\n",
              " '6592_1.txt',\n",
              " '6992_1.txt',\n",
              " '2649_2.txt',\n",
              " '3008_3.txt',\n",
              " '10879_1.txt',\n",
              " '8437_3.txt',\n",
              " '10216_3.txt',\n",
              " '11319_2.txt',\n",
              " '7982_1.txt',\n",
              " '4221_1.txt',\n",
              " '11633_1.txt',\n",
              " '7770_1.txt',\n",
              " '3_4.txt',\n",
              " '9141_1.txt',\n",
              " '3475_3.txt',\n",
              " '706_4.txt',\n",
              " '1693_2.txt',\n",
              " '2464_4.txt',\n",
              " '7094_1.txt',\n",
              " '1662_1.txt',\n",
              " '8733_1.txt',\n",
              " '396_3.txt',\n",
              " '4424_4.txt',\n",
              " '7071_4.txt',\n",
              " '10947_1.txt',\n",
              " '8203_3.txt',\n",
              " '10894_4.txt',\n",
              " '355_4.txt',\n",
              " '7098_4.txt',\n",
              " '3188_1.txt',\n",
              " '10505_2.txt',\n",
              " '6770_1.txt',\n",
              " '3914_2.txt',\n",
              " '545_1.txt',\n",
              " '1113_2.txt',\n",
              " '2583_2.txt',\n",
              " '3045_3.txt',\n",
              " '1379_4.txt',\n",
              " '2388_3.txt',\n",
              " '8747_3.txt',\n",
              " '10318_2.txt',\n",
              " '3654_1.txt',\n",
              " '1990_2.txt',\n",
              " '9012_1.txt',\n",
              " '9769_2.txt',\n",
              " '358_4.txt',\n",
              " '2238_1.txt',\n",
              " '10903_4.txt',\n",
              " '10841_2.txt',\n",
              " '9249_3.txt',\n",
              " '2905_2.txt',\n",
              " '7240_1.txt',\n",
              " '5839_3.txt',\n",
              " '11037_3.txt',\n",
              " '1847_3.txt',\n",
              " '2171_1.txt',\n",
              " '4440_1.txt',\n",
              " '9381_2.txt',\n",
              " '9820_2.txt',\n",
              " '2446_3.txt',\n",
              " '741_1.txt',\n",
              " '9685_4.txt',\n",
              " '1625_1.txt',\n",
              " '5253_3.txt',\n",
              " '8647_1.txt',\n",
              " '4940_1.txt',\n",
              " '9977_4.txt',\n",
              " '7708_1.txt',\n",
              " '7446_2.txt',\n",
              " '6633_1.txt',\n",
              " '8151_4.txt',\n",
              " '11767_4.txt',\n",
              " '10325_1.txt',\n",
              " '1717_3.txt',\n",
              " '3442_2.txt',\n",
              " '8266_4.txt',\n",
              " '9528_2.txt',\n",
              " '12279_1.txt',\n",
              " '8190_4.txt',\n",
              " '7519_2.txt',\n",
              " '5402_4.txt',\n",
              " '5983_4.txt',\n",
              " '8787_4.txt',\n",
              " '3783_4.txt',\n",
              " '8054_1.txt',\n",
              " '9000_2.txt',\n",
              " '11104_1.txt',\n",
              " '7920_1.txt',\n",
              " '10168_2.txt',\n",
              " '3536_1.txt',\n",
              " '10854_4.txt',\n",
              " '7025_1.txt',\n",
              " '8367_1.txt',\n",
              " '361_4.txt',\n",
              " '4281_1.txt',\n",
              " '8912_4.txt',\n",
              " '3609_1.txt',\n",
              " '1033_4.txt',\n",
              " '10142_2.txt',\n",
              " '3958_1.txt',\n",
              " '12396_3.txt',\n",
              " '11420_1.txt',\n",
              " '1633_4.txt',\n",
              " '7108_3.txt',\n",
              " '11792_1.txt',\n",
              " '6717_3.txt',\n",
              " '10075_2.txt',\n",
              " '7579_4.txt',\n",
              " '2538_4.txt',\n",
              " '11209_3.txt',\n",
              " '11230_1.txt',\n",
              " '11628_4.txt',\n",
              " '5872_2.txt',\n",
              " '11289_4.txt',\n",
              " '2888_1.txt',\n",
              " '8805_3.txt',\n",
              " '4539_1.txt',\n",
              " '2115_4.txt',\n",
              " '4764_1.txt',\n",
              " '6823_1.txt',\n",
              " '9455_1.txt',\n",
              " '6219_3.txt',\n",
              " '7204_2.txt',\n",
              " '11366_1.txt',\n",
              " '7553_2.txt',\n",
              " '7677_4.txt',\n",
              " '9222_4.txt',\n",
              " '6588_2.txt',\n",
              " '5186_3.txt',\n",
              " '2292_2.txt',\n",
              " '5193_3.txt',\n",
              " '10626_1.txt',\n",
              " '10992_1.txt',\n",
              " '3345_1.txt',\n",
              " '4587_1.txt',\n",
              " '8140_2.txt',\n",
              " '4669_4.txt',\n",
              " '1631_1.txt',\n",
              " '5786_2.txt',\n",
              " '204_4.txt',\n",
              " '3252_1.txt',\n",
              " '1406_4.txt',\n",
              " '12467_1.txt',\n",
              " '4068_3.txt',\n",
              " '1412_2.txt',\n",
              " '11594_4.txt',\n",
              " '6077_1.txt',\n",
              " '9854_1.txt',\n",
              " '4985_2.txt',\n",
              " '1893_1.txt',\n",
              " '2449_1.txt',\n",
              " '2717_1.txt',\n",
              " '5138_1.txt',\n",
              " '10515_1.txt',\n",
              " '12149_4.txt',\n",
              " '9248_3.txt',\n",
              " '6767_4.txt',\n",
              " '4841_2.txt',\n",
              " '9733_3.txt',\n",
              " '891_2.txt',\n",
              " '4545_1.txt',\n",
              " '8877_4.txt',\n",
              " '12434_4.txt',\n",
              " '1362_4.txt',\n",
              " '4713_3.txt',\n",
              " '3733_1.txt',\n",
              " '8334_1.txt',\n",
              " '5298_2.txt',\n",
              " '3275_1.txt',\n",
              " '9198_1.txt',\n",
              " '8319_4.txt',\n",
              " '8644_1.txt',\n",
              " '4570_1.txt',\n",
              " '7838_1.txt',\n",
              " '4179_4.txt',\n",
              " '6149_1.txt',\n",
              " '8308_1.txt',\n",
              " '5574_4.txt',\n",
              " '1568_1.txt',\n",
              " '1357_4.txt',\n",
              " '6911_1.txt',\n",
              " '481_3.txt',\n",
              " '3813_1.txt',\n",
              " '623_3.txt',\n",
              " '1602_2.txt',\n",
              " '11646_4.txt',\n",
              " '7684_3.txt',\n",
              " '8998_4.txt',\n",
              " '11836_2.txt',\n",
              " '8220_1.txt',\n",
              " '4402_1.txt',\n",
              " '1719_1.txt',\n",
              " '9537_1.txt',\n",
              " '7824_1.txt',\n",
              " '3502_3.txt',\n",
              " '9604_3.txt',\n",
              " '8475_3.txt',\n",
              " '4684_1.txt',\n",
              " '10808_1.txt',\n",
              " '5850_4.txt',\n",
              " '5941_1.txt',\n",
              " '5464_2.txt',\n",
              " '3468_4.txt',\n",
              " '7585_3.txt',\n",
              " '6063_4.txt',\n",
              " '244_4.txt',\n",
              " '9016_1.txt',\n",
              " '6136_2.txt',\n",
              " '2624_1.txt',\n",
              " '8145_1.txt',\n",
              " '1142_3.txt',\n",
              " '11443_2.txt',\n",
              " '7280_1.txt',\n",
              " '4383_1.txt',\n",
              " '8169_4.txt',\n",
              " '8012_1.txt',\n",
              " '4897_2.txt',\n",
              " '3139_1.txt',\n",
              " '12418_4.txt',\n",
              " '3310_4.txt',\n",
              " '1025_2.txt',\n",
              " '372_1.txt',\n",
              " '2607_1.txt',\n",
              " '4134_1.txt',\n",
              " '2336_4.txt',\n",
              " '1420_3.txt',\n",
              " '10225_3.txt',\n",
              " '9450_1.txt',\n",
              " '3230_4.txt',\n",
              " '5787_4.txt',\n",
              " '6180_3.txt',\n",
              " '8271_1.txt',\n",
              " '1191_1.txt',\n",
              " '10157_4.txt',\n",
              " '5209_1.txt',\n",
              " '255_3.txt',\n",
              " '8611_3.txt',\n",
              " '6637_3.txt',\n",
              " '1725_2.txt',\n",
              " '10365_1.txt',\n",
              " '503_2.txt',\n",
              " '2028_3.txt',\n",
              " '3719_3.txt',\n",
              " '11434_2.txt',\n",
              " '6059_1.txt',\n",
              " '7978_1.txt',\n",
              " '738_1.txt',\n",
              " '4308_1.txt',\n",
              " '5764_1.txt',\n",
              " '5037_4.txt',\n",
              " '6815_4.txt',\n",
              " '5725_3.txt',\n",
              " '680_1.txt',\n",
              " '10770_1.txt',\n",
              " '9457_1.txt',\n",
              " '9651_3.txt',\n",
              " '4251_1.txt',\n",
              " '1049_3.txt',\n",
              " '3872_4.txt',\n",
              " '10614_1.txt',\n",
              " '6828_2.txt',\n",
              " '4165_2.txt',\n",
              " '2541_1.txt',\n",
              " '7956_1.txt',\n",
              " '1756_1.txt',\n",
              " '9754_2.txt',\n",
              " '9017_3.txt',\n",
              " '1721_1.txt',\n",
              " '7449_1.txt',\n",
              " '11823_4.txt',\n",
              " '6338_1.txt',\n",
              " '9043_3.txt',\n",
              " '11059_4.txt',\n",
              " '781_1.txt',\n",
              " '1455_1.txt',\n",
              " '8909_2.txt',\n",
              " '9206_3.txt',\n",
              " '2925_1.txt',\n",
              " '905_1.txt',\n",
              " '9332_3.txt',\n",
              " '8753_3.txt',\n",
              " '11917_4.txt',\n",
              " '11571_1.txt',\n",
              " '6390_2.txt',\n",
              " '5856_3.txt',\n",
              " '9303_1.txt',\n",
              " '8074_4.txt',\n",
              " '10799_1.txt',\n",
              " '701_4.txt',\n",
              " '10484_2.txt',\n",
              " '2469_2.txt',\n",
              " '6253_2.txt',\n",
              " '6188_1.txt',\n",
              " '8640_4.txt',\n",
              " '4873_1.txt',\n",
              " '2790_4.txt',\n",
              " '743_1.txt',\n",
              " '5493_3.txt',\n",
              " '2232_1.txt',\n",
              " '3043_1.txt',\n",
              " '6697_2.txt',\n",
              " '10458_2.txt',\n",
              " '5568_3.txt',\n",
              " '5533_1.txt',\n",
              " '12102_4.txt',\n",
              " '614_2.txt',\n",
              " '2187_2.txt',\n",
              " '2674_2.txt',\n",
              " '10926_4.txt',\n",
              " '10437_4.txt',\n",
              " '2169_2.txt',\n",
              " '1124_1.txt',\n",
              " '7074_4.txt',\n",
              " '1358_4.txt',\n",
              " '2029_3.txt',\n",
              " '6302_2.txt',\n",
              " '6202_1.txt',\n",
              " '3250_3.txt',\n",
              " '7474_1.txt',\n",
              " '9983_3.txt',\n",
              " '5631_2.txt',\n",
              " '2295_1.txt',\n",
              " '3474_2.txt',\n",
              " '8160_1.txt',\n",
              " '4015_1.txt',\n",
              " '8403_4.txt',\n",
              " '2190_2.txt',\n",
              " '11849_4.txt',\n",
              " '9182_1.txt',\n",
              " '6928_2.txt',\n",
              " '4967_1.txt',\n",
              " '2889_1.txt',\n",
              " '5867_1.txt',\n",
              " '7188_4.txt',\n",
              " '3245_4.txt',\n",
              " '4175_3.txt',\n",
              " '6756_4.txt',\n",
              " '9177_1.txt',\n",
              " '8468_3.txt',\n",
              " '3564_2.txt',\n",
              " '1889_1.txt',\n",
              " '8965_1.txt',\n",
              " '9165_1.txt',\n",
              " '4797_4.txt',\n",
              " '9860_1.txt',\n",
              " '2923_4.txt',\n",
              " '8359_4.txt',\n",
              " '10333_1.txt',\n",
              " '6399_1.txt',\n",
              " '8592_1.txt',\n",
              " '8155_1.txt',\n",
              " '1036_3.txt',\n",
              " '7852_1.txt',\n",
              " '4195_3.txt',\n",
              " '6736_3.txt',\n",
              " '5587_4.txt',\n",
              " '9722_2.txt',\n",
              " '4690_2.txt',\n",
              " '6154_4.txt',\n",
              " '11603_1.txt',\n",
              " '9897_4.txt',\n",
              " '12002_1.txt',\n",
              " '3267_2.txt',\n",
              " '4759_2.txt',\n",
              " '2003_4.txt',\n",
              " '11585_1.txt',\n",
              " '3890_2.txt',\n",
              " '2203_3.txt',\n",
              " '2287_4.txt',\n",
              " '1217_3.txt',\n",
              " '8136_1.txt',\n",
              " '1438_4.txt',\n",
              " '10346_4.txt',\n",
              " '688_4.txt',\n",
              " '12255_3.txt',\n",
              " '12304_3.txt',\n",
              " '12024_2.txt',\n",
              " '1718_3.txt',\n",
              " '811_1.txt',\n",
              " '5044_1.txt',\n",
              " '3970_2.txt',\n",
              " '7820_1.txt',\n",
              " '893_2.txt',\n",
              " '8899_3.txt',\n",
              " '942_2.txt',\n",
              " '10444_1.txt',\n",
              " '5818_4.txt',\n",
              " '4271_4.txt',\n",
              " '1827_1.txt',\n",
              " '9273_1.txt',\n",
              " '10131_1.txt',\n",
              " '12387_4.txt',\n",
              " '130_1.txt',\n",
              " '2243_4.txt',\n",
              " '4355_1.txt',\n",
              " '2705_1.txt',\n",
              " '12142_1.txt',\n",
              " '7736_2.txt',\n",
              " '2779_1.txt',\n",
              " '6598_1.txt',\n",
              " '6670_4.txt',\n",
              " '665_3.txt',\n",
              " '12444_3.txt',\n",
              " '5578_1.txt',\n",
              " '39_2.txt',\n",
              " '7808_1.txt',\n",
              " '7326_3.txt',\n",
              " '6181_3.txt',\n",
              " '4016_4.txt',\n",
              " '12207_1.txt',\n",
              " '10720_2.txt',\n",
              " '5494_3.txt',\n",
              " '6457_4.txt',\n",
              " '8064_2.txt',\n",
              " '8584_4.txt',\n",
              " '4112_1.txt',\n",
              " '3895_1.txt',\n",
              " '399_2.txt',\n",
              " '3509_4.txt',\n",
              " '8660_1.txt',\n",
              " '631_3.txt',\n",
              " '7869_2.txt',\n",
              " '2314_3.txt',\n",
              " '11676_2.txt',\n",
              " '3655_2.txt',\n",
              " '9287_1.txt',\n",
              " '6492_3.txt',\n",
              " '4158_1.txt',\n",
              " '3533_1.txt',\n",
              " '431_4.txt',\n",
              " '7104_1.txt',\n",
              " '3829_4.txt',\n",
              " '7665_1.txt',\n",
              " '10369_1.txt',\n",
              " '2126_1.txt',\n",
              " '6427_3.txt',\n",
              " '5148_2.txt',\n",
              " '9434_2.txt',\n",
              " '2880_1.txt',\n",
              " '9563_3.txt',\n",
              " '9867_1.txt',\n",
              " '2680_1.txt',\n",
              " '11397_1.txt',\n",
              " '929_1.txt',\n",
              " '12285_4.txt',\n",
              " '4295_2.txt',\n",
              " '6105_2.txt',\n",
              " '6674_3.txt',\n",
              " '11052_1.txt',\n",
              " '7712_2.txt',\n",
              " '8968_1.txt',\n",
              " '6916_2.txt',\n",
              " '6104_1.txt',\n",
              " '7757_1.txt',\n",
              " '12288_4.txt',\n",
              " '1884_3.txt',\n",
              " '7504_4.txt',\n",
              " '4148_1.txt',\n",
              " '709_2.txt',\n",
              " '11521_1.txt',\n",
              " '6997_4.txt',\n",
              " '9214_3.txt',\n",
              " '10845_3.txt',\n",
              " '3612_1.txt',\n",
              " '8741_2.txt',\n",
              " '3538_1.txt',\n",
              " '1508_1.txt',\n",
              " '1774_3.txt',\n",
              " '9025_1.txt',\n",
              " '5968_4.txt',\n",
              " '4752_1.txt',\n",
              " '7227_2.txt',\n",
              " '1712_1.txt',\n",
              " '10700_3.txt',\n",
              " '11574_3.txt',\n",
              " '8262_1.txt',\n",
              " '739_1.txt',\n",
              " '2141_2.txt',\n",
              " '8062_2.txt',\n",
              " '4457_4.txt',\n",
              " '2944_1.txt',\n",
              " '10386_4.txt',\n",
              " '7965_4.txt',\n",
              " '9679_2.txt',\n",
              " '9275_1.txt',\n",
              " '6614_4.txt',\n",
              " '11875_4.txt',\n",
              " '1241_1.txt',\n",
              " '4009_3.txt',\n",
              " '877_3.txt',\n",
              " '3381_1.txt',\n",
              " '707_3.txt',\n",
              " '2299_1.txt',\n",
              " '5951_4.txt',\n",
              " '959_4.txt',\n",
              " '4630_1.txt',\n",
              " '12059_3.txt',\n",
              " '6678_2.txt',\n",
              " '6880_2.txt',\n",
              " '12336_3.txt',\n",
              " '2307_1.txt',\n",
              " '9342_3.txt',\n",
              " '9077_1.txt',\n",
              " '6679_3.txt',\n",
              " '1306_3.txt',\n",
              " '7997_3.txt',\n",
              " '1894_4.txt',\n",
              " '7763_3.txt',\n",
              " '6995_4.txt',\n",
              " '4535_1.txt',\n",
              " '11923_3.txt',\n",
              " '6057_1.txt',\n",
              " '1244_2.txt',\n",
              " '5152_1.txt',\n",
              " '5097_1.txt',\n",
              " '10980_3.txt',\n",
              " '6081_4.txt',\n",
              " '4954_2.txt',\n",
              " '3358_1.txt',\n",
              " '6179_1.txt',\n",
              " '10323_1.txt',\n",
              " '4583_1.txt',\n",
              " '4826_4.txt',\n",
              " '9150_1.txt',\n",
              " '7836_3.txt',\n",
              " '6360_1.txt',\n",
              " '1234_2.txt',\n",
              " '8675_3.txt',\n",
              " '9347_3.txt',\n",
              " '7140_1.txt',\n",
              " '5266_3.txt',\n",
              " '9577_1.txt',\n",
              " '4235_4.txt',\n",
              " '9179_3.txt',\n",
              " '3160_1.txt',\n",
              " '5355_1.txt',\n",
              " '947_2.txt',\n",
              " '10320_2.txt',\n",
              " '7272_4.txt',\n",
              " '4429_1.txt',\n",
              " '6071_4.txt',\n",
              " '234_1.txt',\n",
              " '1020_2.txt',\n",
              " '9885_4.txt',\n",
              " '11087_4.txt',\n",
              " '7183_1.txt',\n",
              " '12035_3.txt',\n",
              " '7120_4.txt',\n",
              " '4559_1.txt',\n",
              " '7890_1.txt',\n",
              " '2079_4.txt',\n",
              " '10086_2.txt',\n",
              " '3060_1.txt',\n",
              " '6776_3.txt',\n",
              " '11484_2.txt',\n",
              " '11067_2.txt',\n",
              " '8294_1.txt',\n",
              " '5874_1.txt',\n",
              " '8817_1.txt',\n",
              " '9594_1.txt',\n",
              " '4127_1.txt',\n",
              " '7905_1.txt',\n",
              " '1141_3.txt',\n",
              " '7656_1.txt',\n",
              " '6382_2.txt',\n",
              " '10371_3.txt',\n",
              " '3307_3.txt',\n",
              " '3467_1.txt',\n",
              " '6631_3.txt',\n",
              " '12492_4.txt',\n",
              " '1489_1.txt',\n",
              " '6522_1.txt',\n",
              " '238_4.txt',\n",
              " '1641_1.txt',\n",
              " '748_1.txt',\n",
              " '2559_1.txt',\n",
              " '4428_1.txt',\n",
              " '6933_1.txt',\n",
              " '6374_3.txt',\n",
              " '7088_1.txt',\n",
              " '3529_2.txt',\n",
              " '3286_1.txt',\n",
              " '6576_1.txt',\n",
              " '5645_2.txt',\n",
              " '2189_2.txt',\n",
              " '8626_1.txt',\n",
              " '1850_1.txt',\n",
              " '8625_4.txt',\n",
              " '9058_3.txt',\n",
              " '1748_1.txt',\n",
              " '5314_1.txt',\n",
              " '4427_1.txt',\n",
              " '2183_2.txt',\n",
              " '1349_3.txt',\n",
              " '12494_1.txt',\n",
              " '4739_3.txt',\n",
              " '7886_1.txt',\n",
              " '1083_1.txt',\n",
              " '6819_3.txt',\n",
              " '10297_3.txt',\n",
              " '8491_4.txt',\n",
              " '2986_4.txt',\n",
              " '11575_1.txt',\n",
              " '6261_2.txt',\n",
              " '5661_2.txt',\n",
              " '1312_3.txt',\n",
              " '4321_1.txt',\n",
              " '9918_1.txt',\n",
              " '10825_1.txt',\n",
              " '11269_1.txt',\n",
              " '10737_3.txt',\n",
              " '10260_1.txt',\n",
              " '7178_1.txt',\n",
              " '4887_2.txt',\n",
              " '5603_1.txt',\n",
              " '10604_2.txt',\n",
              " '8766_1.txt',\n",
              " '2535_2.txt',\n",
              " '6555_2.txt',\n",
              " '3933_4.txt',\n",
              " '10956_2.txt',\n",
              " '6851_4.txt',\n",
              " '2813_1.txt',\n",
              " '4311_3.txt',\n",
              " '813_3.txt',\n",
              " '5174_2.txt',\n",
              " '2972_1.txt',\n",
              " '5570_4.txt',\n",
              " '3236_4.txt',\n",
              " '5372_1.txt',\n",
              " '12318_1.txt',\n",
              " '1348_3.txt',\n",
              " '8028_1.txt',\n",
              " '2562_4.txt',\n",
              " '7279_1.txt',\n",
              " '4653_2.txt',\n",
              " '3768_3.txt',\n",
              " '10115_1.txt',\n",
              " '11882_1.txt',\n",
              " '4458_3.txt',\n",
              " '276_2.txt',\n",
              " '497_2.txt',\n",
              " '3619_3.txt',\n",
              " '1788_3.txt',\n",
              " '949_1.txt',\n",
              " '7171_1.txt',\n",
              " '9852_1.txt',\n",
              " '11584_4.txt',\n",
              " '7315_1.txt',\n",
              " '6235_1.txt',\n",
              " '11374_2.txt',\n",
              " '652_2.txt',\n",
              " '5901_4.txt',\n",
              " '863_2.txt',\n",
              " '239_2.txt',\n",
              " '230_2.txt',\n",
              " '11172_1.txt',\n",
              " '3702_1.txt',\n",
              " '4876_4.txt',\n",
              " '8618_1.txt',\n",
              " '906_1.txt',\n",
              " '11293_3.txt',\n",
              " '5904_3.txt',\n",
              " '10183_1.txt',\n",
              " '11422_1.txt',\n",
              " '4688_3.txt',\n",
              " '7221_1.txt',\n",
              " '8144_3.txt',\n",
              " '2255_4.txt',\n",
              " '6523_1.txt',\n",
              " '5519_4.txt',\n",
              " '5891_3.txt',\n",
              " '6147_1.txt',\n",
              " '11968_4.txt',\n",
              " '10984_2.txt',\n",
              " '129_3.txt',\n",
              " '8130_3.txt',\n",
              " '6342_3.txt',\n",
              " '7786_1.txt',\n",
              " '5736_2.txt',\n",
              " '5829_3.txt',\n",
              " '7796_4.txt',\n",
              " '11685_2.txt',\n",
              " '11979_4.txt',\n",
              " '4446_3.txt',\n",
              " '4947_2.txt',\n",
              " '415_3.txt',\n",
              " '3522_2.txt',\n",
              " '11144_2.txt',\n",
              " '6501_1.txt',\n",
              " '5365_3.txt',\n",
              " '6339_1.txt',\n",
              " '7866_2.txt',\n",
              " '5389_3.txt',\n",
              " '2895_4.txt',\n",
              " '5039_3.txt',\n",
              " '6440_3.txt',\n",
              " '3053_1.txt',\n",
              " '11255_1.txt',\n",
              " '6600_1.txt',\n",
              " '6608_1.txt',\n",
              " '12377_1.txt',\n",
              " '8137_4.txt',\n",
              " '9023_4.txt',\n",
              " '10955_1.txt',\n",
              " '10944_1.txt',\n",
              " '4017_1.txt',\n",
              " '10978_1.txt',\n",
              " '2463_4.txt',\n",
              " '5481_1.txt',\n",
              " '5261_3.txt',\n",
              " '5947_1.txt',\n",
              " '4542_1.txt',\n",
              " '7185_3.txt',\n",
              " '10102_1.txt',\n",
              " '9545_4.txt',\n",
              " '5920_3.txt',\n",
              " '7780_3.txt',\n",
              " '3615_2.txt',\n",
              " '10977_1.txt',\n",
              " '3974_4.txt',\n",
              " '6857_4.txt',\n",
              " '3738_2.txt',\n",
              " '7974_3.txt',\n",
              " '8087_4.txt',\n",
              " '4718_1.txt',\n",
              " '6286_3.txt',\n",
              " '6733_3.txt',\n",
              " '2620_3.txt',\n",
              " '6067_1.txt',\n",
              " '4439_4.txt',\n",
              " '2401_3.txt',\n",
              " '1500_3.txt',\n",
              " '2872_1.txt',\n",
              " '1477_4.txt',\n",
              " '7287_2.txt',\n",
              " '7277_1.txt',\n",
              " '305_1.txt',\n",
              " '4609_4.txt',\n",
              " '5562_2.txt',\n",
              " '1956_3.txt',\n",
              " '10150_3.txt',\n",
              " '4030_4.txt',\n",
              " '10531_3.txt',\n",
              " '1511_1.txt',\n",
              " '9996_4.txt',\n",
              " '11978_4.txt',\n",
              " '7751_4.txt',\n",
              " '1773_3.txt',\n",
              " '12194_3.txt',\n",
              " '4384_1.txt',\n",
              " '10052_4.txt',\n",
              " '2289_3.txt',\n",
              " '8_4.txt',\n",
              " '86_4.txt',\n",
              " '7375_2.txt',\n",
              " '10298_4.txt',\n",
              " '1376_1.txt',\n",
              " '5323_3.txt',\n",
              " '9962_3.txt',\n",
              " '12267_4.txt',\n",
              " ...]"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.listdir(\"aclImdb_v1/aclImdb/train/neg/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QYdNcKffKME"
      },
      "source": [
        "Ahora, con un bucle for leemos todas las reviews negativas y las guardamos en una lista:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4ZCOZSxd-FV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "dir_neg_train = \"/content/aclImdb/train/neg/\"\n",
        "neg_reviews = []\n",
        "\n",
        "for f in os.listdir(dir_neg_train):\n",
        "  with open(f\"{dir_neg_train}/{f}\") as neg:\n",
        "    neg_reviews.append(neg.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFYFOD_OfOUQ"
      },
      "source": [
        "Imprimimos las primeras 3 para corroborar que nuestro código funcione bien:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN-uU0tGfCWQ",
        "outputId": "25165a62-e4cb-40e5-8517-232ae428df74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Don\\'t drink the cool-aid.<br /><br />This is an opinion piece disguised as a documentary. And to title it as a \"truth\" is just plain crap. The debate over global warming is far from over, and will only be over when the eco-zombies start acknowledging the mountain of evidence contrary to their beloved theory. Just Google \"Global Warming\" and \"Hoax\" or \"Junk Science\" and you will find a river of information refuting nearly every link in the chain of logic that Gore sites. The reason it is so important for people to educate themselves is the disastrous economic impact that global warming prevention measures would have. Wake up people. Anyone with a computer, a little time, and some common sense can find many many reasons why this theory is not even close to credible. Don\\'t just read articles that support your present opinions, read everything you can find. There is no in-depth analysis to make, really. There is simply too many alternate possibilities and counter-evidence for the theory to have even the most basic level of scientific credibility. It is so uncredible, in fact, that it may be the single biggest hoax in the course of human existence. It\\'s time for people to start speaking out against this kind of propaganda, and it\\'s time for people to admit to themselves and others that you can be a both a conservationist AND recognize the glaring conclusion that global warming hysteria is a big lie.',\n",
              " 'Typical Troma-trash, this smutty 80\\'s flick is considered one of the \"highlights\" of Lloyd Kaufman\\'s notorious production studio, alongside \"The Toxic Avenger\" released one year earlier. \"The Toxic Avenger\" is far superior if you ask me, but this demented splatter-flick is nevertheless endurable as well; just make sure you leave your full brain capacity at the door. The events take place in Tromaville, a little town that proudly claims to be the toxic chemical capital of the world, and they certainly aren\\'t lying. The safety precautions in the local nuclear power plant are substandard, to say the least (even Homer Simpson never was this nonchalant) and toxic waste seeps through to the nearby high school. The first intoxicated victim is the stereotypical nerd, who starts spurting green stuff out of all his body cavities, but his death is believed to be an accident because he had no less than TWO microwave ovens in his house! Oh, the humanity! Shortly after, however, the nuclear leaks also affect the school\\'s weed plantation and thing really start to get messy. After smoking a joint at a party, the cutest couple in school produce a gigantic worm monster that settles in the basement and feeds on teenage scum. \"Class of Nuke \\'em High\" is bottom-of-the-barrel horror film-making, with dialogs so dumb they hurt your ears and make-up effects that give a whole new meaning to the word tasteless. If you enjoy watching faces melting away, getting crushed or splitting in half, this is definitely a must-see! Unlike the aforementioned \"The Toxic Avenger\", this film suffers from a couple of really dull and overlong moments where nothing really significant happens, like for example when Chrissy and Warren try to figure out what\\'s wrong with their hormones. The crude humor isn\\'t as effective as in \"Toxic Avenger\" and the acting performances are unforgivably amateurish. Proceed only if you\\'re an avid Troma-fanatic.',\n",
              " 'From the blocky digitised footage to the acting that makes Keanu \"I\\'m so wooden I could be a Plank me\" Reeves look like an Oscar winner this film bites (pun not intended). The best thing about it is the box of eRATicate in the 2nd segment (which out of the three seemed to be the strongest piece in terms of storyline and \\'twist\\'). Wish I\\'d spent the £3.99 it cost me on something else, like erm.... Natural Born Killers: Directors Cut. If you do buy this, you\\'re really in for a disappointment, do yourself a favour and avoid it like the plague. If you\\'re looking for something amateurish and with actors that are more wooden than a 2x4 then go ahead. However if you want some quality werewolf action look elsewhere, like Dog Soldiers, Wolfen, Romasanta:The werewolf Hunt.']"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "neg_reviews[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITRWLtdAfvva",
        "outputId": "d4e8951c-5e01-4c17-fa7e-fce5cd985e31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12500"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(neg_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcEGxvyKfSgH"
      },
      "source": [
        "Ahora, debemos hacer lo mismo con las negativas de test y luego con las positivas.\n",
        "\n",
        "En nuestro caso, vamos a hacer nuestro propio train/test split, por lo que las a reviews que están en el directorio de test las guardaremos en la misma lista que recién.\n",
        "\n",
        "Agregar a la lista \"neg_reviews\" las reviews negativas de test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTPPmnXifkpL"
      },
      "outputs": [],
      "source": [
        "dir_neg_test = \"aclImdb_v1/aclImdb/test/neg/\"\n",
        "for f in os.listdir(dir_neg_test):\n",
        "  with open(f\"{dir_neg_test}/{f}\") as neg:\n",
        "    neg_reviews.append(neg.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrqDiB9Cf1Ip"
      },
      "source": [
        "Imprimir el largo de la nueva lista para corroborar que se hayan agregado todas las reviews (deberían tener 25mil)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq2_b94Ofx2U",
        "outputId": "1a867e65-3388-461c-8189-c949c9396709"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(neg_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBf3K0obf-CX"
      },
      "source": [
        "Ahora, hacer lo mismo con las positivas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUQ6ppWjgBZ7",
        "outputId": "7b29d739-edbe-419d-ede7-4f01c5c63a92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"Josef Von Sternberg directs this magnificent silent film about silent Hollywood and the former Imperial General to the Czar of Russia who has found himself there. Emil Jannings won a well-deserved Oscar, in part, for his role as the general who ironically is cast in a bit part in a silent picture as a Russian general. The movie flashes back to his days in Russia leading up to the country's fall to revolutionaries. William Powell makes his big screen debut as the Hollywood director who casts Jannings in his film. The film serves as an interesting look at the fall of Russia and at an imitation of behind-the-scenes Tinseltown in the early days. Von Sternberg delivers yet another classic, and one that is filled with the great elements of romance, intrigue, and tragedy.\", 'This is such a great movie \"Call Me Anna\" because it shows how a person has suffered for so long without knowing what was wrong with her. For Patty Duke to come out in the publics eye and tell her story is an inspiration to those who suffer from this disease. I have a lot of respect for her as a person. The only thing I don\\'t like is I can\\'t get it on tape, I\\'ve tried looking for it but with no success. Any one know how to get it?', \"I'd waited for some years before this movie finally got released in England, but was in many ways very pleased when I finally saw it. There are a lot of great things to the film, for a start the acting. Its not something I have all that much need for in a horror picture but the people in this film all put in fine work. This and the constantly gripping and interesting script, with a nice sorta Lovecraftian feel to it, give the film a real solid backbone. Add to this the doses of surreal nightmare imagery and occasional gruesome gore and the films a winner. It has my favorite kind of gore too, supernatural and splattery. Also, the characters of Marcus, the angry bodybuilding transsexual and Daisy, his mentally retarded lover/plaything are genuinely freakish and unnerving at times, and give a far out, anything goes sense of morbid grown up craziness which works well with the frequent Freudian overtones. This is one of the most impressive recent horror movies, far more shocking or out there than anything Hollywood can produce. My only gripe was that I wanted the ending to be darker in tone, but it still works, so on the whole I'd really recommend this to serious horror buffs.\"]\n",
            "25000\n"
          ]
        }
      ],
      "source": [
        "dir_pos_train = \"aclImdb_v1/aclImdb/train/pos/\"\n",
        "dir_pos_test = \"aclImdb_v1/aclImdb/test/pos/\"\n",
        "pos_reviews = []\n",
        "\n",
        "for f in os.listdir(dir_pos_train):\n",
        "  with open(f\"{dir_pos_train}/{f}\") as pos:\n",
        "    pos_reviews.append(pos.read())\n",
        "\n",
        "for f in os.listdir(dir_pos_test):\n",
        "  with open(f\"{dir_pos_test}/{f}\") as pos:\n",
        "    pos_reviews.append(pos.read())\n",
        "\n",
        "print(pos_reviews[:3])\n",
        "print(len(pos_reviews))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6huetZ7gVcQ"
      },
      "source": [
        "Deberían tener 25mil reviews de cada tipo.\n",
        "\n",
        "Ahora, almacenaremos estos datos en un dataframe de pandas para trabajar de forma más simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tqoNArgEgBcq",
        "outputId": "e33e5e48-b9ba-4ea5-c4b4-f6fc307062e1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>REVIEW</th>\n",
              "      <th>TARGET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Josef Von Sternberg directs this magnificent s...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is such a great movie \"Call Me Anna\" beca...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I'd waited for some years before this movie fi...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Allow me to just get to the bottom line here: ...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>An excellent movie and great example of how sc...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              REVIEW TARGET\n",
              "0  Josef Von Sternberg directs this magnificent s...    POS\n",
              "1  This is such a great movie \"Call Me Anna\" beca...    POS\n",
              "2  I'd waited for some years before this movie fi...    POS\n",
              "3  Allow me to just get to the bottom line here: ...    POS\n",
              "4  An excellent movie and great example of how sc...    POS"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos_df = pd.DataFrame(pos_reviews, columns=[\"REVIEW\"])\n",
        "pos_df[\"TARGET\"] = \"POS\"\n",
        "neg_df = pd.DataFrame(neg_reviews, columns=[\"REVIEW\"])\n",
        "neg_df[\"TARGET\"] = \"NEG\"\n",
        "\n",
        "df = pd.concat([pos_df, neg_df], axis=\"rows\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWQ0JWgBg8Kw",
        "outputId": "6d6f4f34-bed3-4880-d0ef-fee868ef29f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "27WPYFIng9Yc",
        "outputId": "e1fcfeaa-2bb2-4667-c61c-2cb442a3ec62"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>REVIEW</th>\n",
              "      <th>TARGET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14435</th>\n",
              "      <td>First of all, I would like to say that I am a ...</td>\n",
              "      <td>NEG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21955</th>\n",
              "      <td>What can i say about Tromeo and Juliet, other ...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21546</th>\n",
              "      <td>Very good film. Very good documentary.&lt;br /&gt;&lt;b...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5316</th>\n",
              "      <td>Earlier today I got into an argument on why so...</td>\n",
              "      <td>NEG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20318</th>\n",
              "      <td>I saw this movie on a fluke.I was standing on ...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  REVIEW TARGET\n",
              "14435  First of all, I would like to say that I am a ...    NEG\n",
              "21955  What can i say about Tromeo and Juliet, other ...    POS\n",
              "21546  Very good film. Very good documentary.<br /><b...    POS\n",
              "5316   Earlier today I got into an argument on why so...    NEG\n",
              "20318  I saw this movie on a fluke.I was standing on ...    POS"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erwfEkU4hsEQ"
      },
      "source": [
        "Ya tenemos nuestro dataframe listo para trabajar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbzm2R8shxBT"
      },
      "source": [
        "### Spacy\n",
        "\n",
        "Como dijimos anteriormente, trabajaremos con la librería spacy. Siempre debemos importarla e instanciarla llamando a nuestro lenguaje. Si queremos instanciarla con un lenguaje que no tenemos descargado, nos dará un error y ahi podemos copiar y pegar el código para descargar el mismo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5eNfBDBiDyn"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbOqyWOqiE_w"
      },
      "source": [
        "Para tokenizar un texto en spacy, simplemente utilizamos el objeto que instanciamos (que en este caso llamamos nlp)\n",
        "\n",
        "Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uBGPCrTiP6c",
        "outputId": "3f261158-9551-41ca-d860-e080da8d08f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Hola como estás"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(\"Hola como estás\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhcASYYBiTxJ"
      },
      "source": [
        "Si queremos acceder a cada uno de los tokens, podemos utilizar por ejemplo un bucle for:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpJsY2UKiXdT",
        "outputId": "a282d7bf-7c48-478c-ac66-314c8886f130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola\n",
            "---\n",
            "como\n",
            "---\n",
            "estás\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "for token in nlp(\"Hola como estás\"):\n",
        "  print(token)\n",
        "  print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAxeHlY_ikto"
      },
      "source": [
        "### Stop words\n",
        "\n",
        "En spacy, tenemos para cada idioma un listado de stop words por defecto (que podemos modificar agregando o quitando las que necesitemos).\n",
        "\n",
        "Ejecutando la siguiente celda, podemos ver el listado que viene por defecto para el idioma inglés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctIVpG8iisKY",
        "outputId": "c6455b0b-fbc5-42a2-f3d4-f197a45a446d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\"'d\",\n",
              " \"'ll\",\n",
              " \"'m\",\n",
              " \"'re\",\n",
              " \"'s\",\n",
              " \"'ve\",\n",
              " 'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'across',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'again',\n",
              " 'against',\n",
              " 'all',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'am',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amount',\n",
              " 'an',\n",
              " 'and',\n",
              " 'another',\n",
              " 'any',\n",
              " 'anyhow',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'anywhere',\n",
              " 'are',\n",
              " 'around',\n",
              " 'as',\n",
              " 'at',\n",
              " 'back',\n",
              " 'be',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'been',\n",
              " 'before',\n",
              " 'beforehand',\n",
              " 'behind',\n",
              " 'being',\n",
              " 'below',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'both',\n",
              " 'bottom',\n",
              " 'but',\n",
              " 'by',\n",
              " 'ca',\n",
              " 'call',\n",
              " 'can',\n",
              " 'cannot',\n",
              " 'could',\n",
              " 'did',\n",
              " 'do',\n",
              " 'does',\n",
              " 'doing',\n",
              " 'done',\n",
              " 'down',\n",
              " 'due',\n",
              " 'during',\n",
              " 'each',\n",
              " 'eight',\n",
              " 'either',\n",
              " 'eleven',\n",
              " 'else',\n",
              " 'elsewhere',\n",
              " 'empty',\n",
              " 'enough',\n",
              " 'even',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'everywhere',\n",
              " 'except',\n",
              " 'few',\n",
              " 'fifteen',\n",
              " 'fifty',\n",
              " 'first',\n",
              " 'five',\n",
              " 'for',\n",
              " 'former',\n",
              " 'formerly',\n",
              " 'forty',\n",
              " 'four',\n",
              " 'from',\n",
              " 'front',\n",
              " 'full',\n",
              " 'further',\n",
              " 'get',\n",
              " 'give',\n",
              " 'go',\n",
              " 'had',\n",
              " 'has',\n",
              " 'have',\n",
              " 'he',\n",
              " 'hence',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hereafter',\n",
              " 'hereby',\n",
              " 'herein',\n",
              " 'hereupon',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'however',\n",
              " 'hundred',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'indeed',\n",
              " 'into',\n",
              " 'is',\n",
              " 'it',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'keep',\n",
              " 'last',\n",
              " 'latter',\n",
              " 'latterly',\n",
              " 'least',\n",
              " 'less',\n",
              " 'made',\n",
              " 'make',\n",
              " 'many',\n",
              " 'may',\n",
              " 'me',\n",
              " 'meanwhile',\n",
              " 'might',\n",
              " 'mine',\n",
              " 'more',\n",
              " 'moreover',\n",
              " 'most',\n",
              " 'mostly',\n",
              " 'move',\n",
              " 'much',\n",
              " 'must',\n",
              " 'my',\n",
              " 'myself',\n",
              " \"n't\",\n",
              " 'name',\n",
              " 'namely',\n",
              " 'neither',\n",
              " 'never',\n",
              " 'nevertheless',\n",
              " 'next',\n",
              " 'nine',\n",
              " 'no',\n",
              " 'nobody',\n",
              " 'none',\n",
              " 'noone',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'nothing',\n",
              " 'now',\n",
              " 'nowhere',\n",
              " 'n‘t',\n",
              " 'n’t',\n",
              " 'of',\n",
              " 'off',\n",
              " 'often',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'only',\n",
              " 'onto',\n",
              " 'or',\n",
              " 'other',\n",
              " 'others',\n",
              " 'otherwise',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 'part',\n",
              " 'per',\n",
              " 'perhaps',\n",
              " 'please',\n",
              " 'put',\n",
              " 'quite',\n",
              " 'rather',\n",
              " 're',\n",
              " 'really',\n",
              " 'regarding',\n",
              " 'same',\n",
              " 'say',\n",
              " 'see',\n",
              " 'seem',\n",
              " 'seemed',\n",
              " 'seeming',\n",
              " 'seems',\n",
              " 'serious',\n",
              " 'several',\n",
              " 'she',\n",
              " 'should',\n",
              " 'show',\n",
              " 'side',\n",
              " 'since',\n",
              " 'six',\n",
              " 'sixty',\n",
              " 'so',\n",
              " 'some',\n",
              " 'somehow',\n",
              " 'someone',\n",
              " 'something',\n",
              " 'sometime',\n",
              " 'sometimes',\n",
              " 'somewhere',\n",
              " 'still',\n",
              " 'such',\n",
              " 'take',\n",
              " 'ten',\n",
              " 'test',\n",
              " 'than',\n",
              " 'that',\n",
              " 'the',\n",
              " 'their',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'thence',\n",
              " 'there',\n",
              " 'thereafter',\n",
              " 'thereby',\n",
              " 'therefore',\n",
              " 'therein',\n",
              " 'thereupon',\n",
              " 'these',\n",
              " 'they',\n",
              " 'third',\n",
              " 'this',\n",
              " 'those',\n",
              " 'though',\n",
              " 'three',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'thru',\n",
              " 'thus',\n",
              " 'to',\n",
              " 'together',\n",
              " 'too',\n",
              " 'top',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'twelve',\n",
              " 'twenty',\n",
              " 'two',\n",
              " 'under',\n",
              " 'unless',\n",
              " 'until',\n",
              " 'up',\n",
              " 'upon',\n",
              " 'us',\n",
              " 'used',\n",
              " 'using',\n",
              " 'various',\n",
              " 'very',\n",
              " 'via',\n",
              " 'was',\n",
              " 'we',\n",
              " 'well',\n",
              " 'were',\n",
              " 'what',\n",
              " 'whatever',\n",
              " 'when',\n",
              " 'whence',\n",
              " 'whenever',\n",
              " 'where',\n",
              " 'whereafter',\n",
              " 'whereas',\n",
              " 'whereby',\n",
              " 'wherein',\n",
              " 'whereupon',\n",
              " 'wherever',\n",
              " 'whether',\n",
              " 'which',\n",
              " 'while',\n",
              " 'whither',\n",
              " 'who',\n",
              " 'whoever',\n",
              " 'whole',\n",
              " 'whom',\n",
              " 'whose',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'within',\n",
              " 'without',\n",
              " 'would',\n",
              " 'yet',\n",
              " 'you',\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " '‘d',\n",
              " '‘ll',\n",
              " '‘m',\n",
              " '‘re',\n",
              " '‘s',\n",
              " '‘ve',\n",
              " '’d',\n",
              " '’ll',\n",
              " '’m',\n",
              " '’re',\n",
              " '’s',\n",
              " '’ve'}"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp.Defaults.stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE48pPstjnHo"
      },
      "source": [
        "Si queremos agregar una stopword, podemos hacerlo con el método .add() de las listas.\n",
        "\n",
        "Por ejemplo, imaginen que queremos agregar la palabra \"test\".\n",
        "\n",
        "Primero validamos si existe en la lista:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zi4HCppisNJ",
        "outputId": "38bd77e7-5003-4d46-addb-1661f2cfa5fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"test\" in nlp.Defaults.stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngXA8Vgtjxpn"
      },
      "source": [
        "No existe, la agreguemos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb8VUm6ljzbS"
      },
      "outputs": [],
      "source": [
        "nlp.Defaults.stop_words.add(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZJFEzl-j0yM",
        "outputId": "634bb83b-cb1e-42ee-8add-42ae0679d015"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"test\" in nlp.Defaults.stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbHe7YbWj2Bc"
      },
      "source": [
        "Ahora si existe.\n",
        "\n",
        "Para saber si un token es una stopword o no, podemos utilizar el atributo is_stop de un token.\n",
        "\n",
        "Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQcZ8msPj00k",
        "outputId": "27d768a5-fbcf-41d0-c088-2c227bbc2462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La palabra: My es una stop word.\n",
            "La palabra: name es una stop word.\n",
            "La palabra: is es una stop word.\n",
            "La palabra: I es una stop word.\n",
            "La palabra: am es una stop word.\n",
            "La palabra: and es una stop word.\n",
            "La palabra: I es una stop word.\n",
            "La palabra: in es una stop word.\n"
          ]
        }
      ],
      "source": [
        "for token in nlp(\"My name is Alexis. I am 25 years old and I live in Bs As.\"):\n",
        "  if token.is_stop:\n",
        "    print(f\"La palabra: {token.text} es una stop word.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb9UVPmykShs"
      },
      "source": [
        "De esta forma, podemos limpiar las stop words de un texto. Veamos un ejemplo en el que limpiamos las stop words del texto \"My name is Federico. I am 24 years old and I live in Córdoba.\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UFUPj1FkaFB"
      },
      "outputs": [],
      "source": [
        "def clean_stop_words(text):\n",
        "  clean_text = []\n",
        "  for token in nlp(text):\n",
        "    if not token.is_stop:\n",
        "      clean_text.append(token.text)\n",
        "\n",
        "  return \" \".join(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "coEMoUgakaNm",
        "outputId": "6da70997-07f8-48e0-efaf-10820ebf437c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Federico . 24 years old live Córdoba .'"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texto = \"My name is Alexis. I am 25 years old and I live in Bs As.\"\n",
        "\n",
        "clean_stop_words(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olZ-lZfClCXl"
      },
      "source": [
        "Vemos que nos limpio las stop words, pero además necesitaríamos pasar el texto a minúsculas y eliminar los signos de puntuación.\n",
        "\n",
        "Para lo primero, podemos utilizar la función lower() de python.\n",
        "\n",
        "Para lo segundo, los tokens tienen el atributo token.is_punct.\n",
        "\n",
        "EJERCICIO: Crear una nueva función (basada en la que definimos recien) que se llame clean_text y además de eliminar stop words, elimine signos de puntuación y convierta todo a minúsculas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op9afpZOlbZB"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  clean_text = []\n",
        "  for token in nlp(text):\n",
        "    if not token.is_stop and not token.is_punct:\n",
        "      clean_text.append(token.text.lower())\n",
        "\n",
        "  return \" \".join(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YkBSCGGlmTy"
      },
      "source": [
        "Probamos la función con el mismo texto que recién:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AIVndVfFljTW",
        "outputId": "87ecab23-c020-48fa-ca56-6adf7971eedb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'federico 24 years old live córdoba'"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texto = \"My name is Alexis. I am 25 years old and I live in Bs As.\"\n",
        "\n",
        "clean_text(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIdpyRYnlpja"
      },
      "source": [
        "Ahora, si quisiéramos aplicar esta función a nuestro dataset entero, como lo haríamos???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gVD0vgklxGZ"
      },
      "source": [
        "### Raiz (lemma)\n",
        "\n",
        "En spacy, también podemos llevar a las palabras a su raiz de una forma muy simple utilizando el atributo .lemma_ (recuerden que finaliza con _) de los tokens.\n",
        "\n",
        "Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5gtZ2ZTl5eG",
        "outputId": "79938fac-3851-4ebf-f849-70dadde598de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL Reeves, LEMMA: reeve\n",
            "ORIGINAL look, LEMMA: look\n",
            "ORIGINAL like, LEMMA: like\n",
            "ORIGINAL an, LEMMA: an\n",
            "ORIGINAL Oscar, LEMMA: Oscar\n",
            "ORIGINAL winner, LEMMA: winner\n",
            "ORIGINAL this, LEMMA: this\n",
            "ORIGINAL film, LEMMA: film\n",
            "ORIGINAL bites, LEMMA: bite\n",
            "ORIGINAL (, LEMMA: (\n",
            "ORIGINAL pun, LEMMA: pun\n",
            "ORIGINAL not, LEMMA: not\n",
            "ORIGINAL intended, LEMMA: intend\n",
            "ORIGINAL ), LEMMA: )\n",
            "ORIGINAL ., LEMMA: .\n",
            "ORIGINAL \n",
            ", LEMMA: \n",
            "\n",
            "ORIGINAL The, LEMMA: the\n",
            "ORIGINAL best, LEMMA: good\n",
            "ORIGINAL thing, LEMMA: thing\n",
            "ORIGINAL about, LEMMA: about\n",
            "ORIGINAL it, LEMMA: -PRON-\n",
            "ORIGINAL is, LEMMA: be\n",
            "ORIGINAL the, LEMMA: the\n",
            "ORIGINAL box, LEMMA: box\n",
            "ORIGINAL of, LEMMA: of\n",
            "ORIGINAL eRATicate, LEMMA: eRATicate\n",
            "ORIGINAL in, LEMMA: in\n",
            "ORIGINAL the, LEMMA: the\n",
            "ORIGINAL 2nd, LEMMA: 2nd\n",
            "ORIGINAL segment, LEMMA: segment\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Reeves looks like an Oscar winner this film bites (pun not intended). \n",
        "The best thing about it is the box of eRATicate in the 2nd segment\"\"\"\n",
        "\n",
        "for token in nlp(text):\n",
        "    print(f\"ORIGINAL {token.text}, LEMMA: {token.lemma_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNFesN3RmoQ0"
      },
      "source": [
        "EJERCICIO: A la función clean text, agregarle que convierta el texto a lemma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTtKK3YfmseV"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  clean_text = []\n",
        "  for token in nlp(text):\n",
        "    if not token.is_stop and not token.is_punct:\n",
        "      clean_text.append(token.lemma_.lower())\n",
        "\n",
        "  return \" \".join(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BgOjb6Lkmsol",
        "outputId": "09400548-17c7-41bb-97c0-c4c9b90ec2f0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'reeve look like oscar winner film bite pun intend \\n good thing box eraticate 2nd segment'"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"\"\"Reeves look like an Oscar winner this film bites (pun not intended). \n",
        "The best thing about it is the box of eRATicate in the 2nd segment\"\"\"\n",
        "clean_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hboeBtrkm0lu"
      },
      "source": [
        "Vemos que todavía quedan caracteres especiales como por ejemplo \\n. Ya veremos en próximas clases como limpiar este tipo de elementos utilizando expresiones regulares. \n",
        "\n",
        "Por ahora, podemos utilizar la función .replace() de los strings. Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SX7XDZrvnBtP",
        "outputId": "8e303ff9-0ae0-4998-b487-9addd07d6ea3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hola  estas'"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"hola como estas\".replace(\"como\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usJI8td5nGDN"
      },
      "source": [
        "### Bag of words\n",
        "\n",
        "Para vectorizar con bag of words, utilizaremos sklearn. \n",
        "\n",
        "En sklearn, este elemento se llama CountVectorizer.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "\n",
        "Sigue la lógica de fit/transform.\n",
        "\n",
        "Podemos ver algunos parámetros importantes como por ejemplo:\n",
        "\n",
        "- ngram_range\n",
        "- lowercase\n",
        "- stop_words\n",
        "- strip_accents\n",
        "\n",
        "\n",
        "Antes de aplicar count vectorizer sobre nuestro df, vamos a aplicarle nuestra función \"clean_text\".\n",
        "\n",
        "EJERCICIO: Aplicar clean_text a todo nuestro dataframe. \n",
        "\n",
        "Este proceso puede tomar más de media hs para el dataset que tenemos, por lo tanto, nos quedaremos únicamente con las primeras 5mil filas para poder ejecutar el código en clases. Luego ustedes pueden probarlo con el dataset completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuoAZ-UJktQ1"
      },
      "outputs": [],
      "source": [
        "# COMENTAR ESTA CELDA SI QUIEREN TRABAJAR CON EL DATASET COMPLETO (les puede tomar 30 min el preprocesamiento)\n",
        "pos_samples = df[df.TARGET=='POS'].head(2500)\n",
        "neg_samples = df[df.TARGET=='NEG'].head(2500)\n",
        "\n",
        "df = pd.concat([pos_samples, neg_samples])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUMK5jsUn7aP",
        "outputId": "2f376b1f-d026-462b-8c6d-91b956a7d4b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 1s, sys: 2.32 s, total: 4min 4s\n",
            "Wall time: 4min 3s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "df[\"REVIEW\"] = df[\"REVIEW\"].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Z9eZadM8oBoi",
        "outputId": "a026400b-4218-4c37-b294-90cfb9c9ee90"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>REVIEW</th>\n",
              "      <th>TARGET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>josef von sternberg direct magnificent silent ...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>great movie anna show person suffer long know ...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wait year movie finally get release england wa...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>allow line get 3 kid age 5 10 consider trip th...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>excellent movie great example scary movie show...</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              REVIEW TARGET\n",
              "0  josef von sternberg direct magnificent silent ...    POS\n",
              "1  great movie anna show person suffer long know ...    POS\n",
              "2  wait year movie finally get release england wa...    POS\n",
              "3  allow line get 3 kid age 5 10 consider trip th...    POS\n",
              "4  excellent movie great example scary movie show...    POS"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkoTchHjoU7E"
      },
      "source": [
        "Ahora, debemos hacer train_test_split. Utilizar como random_state 0 y test_size de 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8x4gbLXoaeS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.REVIEW.copy()\n",
        "y = df.TARGET.copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbyEjPgvoJTS"
      },
      "source": [
        "Ahora si, importemos count vectorizer y lo apliquemos en nuestro texto.\n",
        "\n",
        "Recuerden: fit solo sobre train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkYN0pSEn-T3"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQApQC-Dn7c9"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer()\n",
        "\n",
        "cv.fit(X_train)\n",
        "\n",
        "X_train = cv.transform(X_train)\n",
        "X_test = cv.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdsO-RGhpafC",
        "outputId": "ae4fc329-f0b3-4c50-d0bd-3733c09465ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<4000x29434 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 342225 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NY8tatypbgc",
        "outputId": "ce31fd26-95bd-4ab3-dd1f-cd3e01c41408"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1000x29434 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 81583 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7kN9Zy7o4vu"
      },
      "source": [
        "Ahora, además podemos ver las \"features\" con la siguiente función de nuestro count vectorizer (en la siguiente celda, cambien el nombre del vectorizer que ustedes hayan utilizado)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bfj1_JCn7gZ",
        "outputId": "b69fd05c-f0c7-41a9-fb9b-c117492a7f0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['00',\n",
              " '000',\n",
              " '00001',\n",
              " '006',\n",
              " '007',\n",
              " '0079',\n",
              " '0080',\n",
              " '0083',\n",
              " '00pm',\n",
              " '01',\n",
              " '02',\n",
              " '03',\n",
              " '04',\n",
              " '05',\n",
              " '06',\n",
              " '07',\n",
              " '08',\n",
              " '0ne',\n",
              " '10',\n",
              " '100',\n",
              " '1000',\n",
              " '1001',\n",
              " '100m',\n",
              " '100th',\n",
              " '101',\n",
              " '102',\n",
              " '102nd',\n",
              " '103',\n",
              " '104',\n",
              " '1040s',\n",
              " '105',\n",
              " '105lbs',\n",
              " '106',\n",
              " '108',\n",
              " '109',\n",
              " '10pm',\n",
              " '10star',\n",
              " '10th',\n",
              " '11',\n",
              " '110',\n",
              " '1100',\n",
              " '11001001',\n",
              " '112',\n",
              " '115',\n",
              " '116',\n",
              " '11th',\n",
              " '12',\n",
              " '120',\n",
              " '12383499143743701',\n",
              " '125',\n",
              " '12th',\n",
              " '13',\n",
              " '1300',\n",
              " '131',\n",
              " '132',\n",
              " '134',\n",
              " '135',\n",
              " '137',\n",
              " '13k',\n",
              " '13th',\n",
              " '14',\n",
              " '140',\n",
              " '146',\n",
              " '149',\n",
              " '1492',\n",
              " '14a',\n",
              " '14th',\n",
              " '15',\n",
              " '150',\n",
              " '1500',\n",
              " '157',\n",
              " '15mins',\n",
              " '15minutes',\n",
              " '16',\n",
              " '160lbs',\n",
              " '163',\n",
              " '16mm',\n",
              " '16th',\n",
              " '16ème',\n",
              " '17',\n",
              " '1775',\n",
              " '1798',\n",
              " '17th',\n",
              " '18',\n",
              " '1800',\n",
              " '1812',\n",
              " '1820',\n",
              " '1824',\n",
              " '1830',\n",
              " '1836',\n",
              " '1837',\n",
              " '1838',\n",
              " '1840',\n",
              " '1846',\n",
              " '1847',\n",
              " '1850',\n",
              " '1853',\n",
              " '1854',\n",
              " '1855',\n",
              " '1860',\n",
              " '1861',\n",
              " '1865',\n",
              " '1873',\n",
              " '1876',\n",
              " '188',\n",
              " '1880',\n",
              " '1888',\n",
              " '1890',\n",
              " '1890s',\n",
              " '1895',\n",
              " '1896',\n",
              " '1898',\n",
              " '18a',\n",
              " '18th',\n",
              " '19',\n",
              " '1900',\n",
              " '1900s',\n",
              " '1901',\n",
              " '1902',\n",
              " '1903',\n",
              " '1905',\n",
              " '1906',\n",
              " '1907',\n",
              " '1909',\n",
              " '1913',\n",
              " '1914',\n",
              " '1917',\n",
              " '1919',\n",
              " '192',\n",
              " '1920',\n",
              " '1920s',\n",
              " '1921',\n",
              " '1922',\n",
              " '1923',\n",
              " '1924',\n",
              " '1925',\n",
              " '1927',\n",
              " '1928',\n",
              " '1929',\n",
              " '1930',\n",
              " '1930s',\n",
              " '1931',\n",
              " '1932',\n",
              " '1933',\n",
              " '1934',\n",
              " '1935',\n",
              " '1936',\n",
              " '1937',\n",
              " '1938',\n",
              " '1939',\n",
              " '194',\n",
              " '1940',\n",
              " '1940s',\n",
              " '1941',\n",
              " '1942',\n",
              " '1944',\n",
              " '1945',\n",
              " '1946',\n",
              " '1947',\n",
              " '1948',\n",
              " '1949',\n",
              " '195',\n",
              " '1950',\n",
              " '1950s',\n",
              " '1951',\n",
              " '1952',\n",
              " '1953',\n",
              " '1954',\n",
              " '1955',\n",
              " '1956',\n",
              " '1957',\n",
              " '1958',\n",
              " '1959',\n",
              " '1960',\n",
              " '1960s',\n",
              " '1961',\n",
              " '1962',\n",
              " '1963',\n",
              " '1964',\n",
              " '1965',\n",
              " '1966',\n",
              " '1967',\n",
              " '1968',\n",
              " '1969',\n",
              " '1970',\n",
              " '1970ie',\n",
              " '1970s',\n",
              " '1971',\n",
              " '1972',\n",
              " '1973',\n",
              " '1974',\n",
              " '1975',\n",
              " '1976',\n",
              " '1977',\n",
              " '1978',\n",
              " '1979',\n",
              " '19796',\n",
              " '1980',\n",
              " '1980s',\n",
              " '1981',\n",
              " '1982',\n",
              " '1983',\n",
              " '1984',\n",
              " '1985',\n",
              " '1986',\n",
              " '1987',\n",
              " '1988',\n",
              " '1989',\n",
              " '1990',\n",
              " '1990s',\n",
              " '1991',\n",
              " '1992',\n",
              " '1993',\n",
              " '1994',\n",
              " '1995',\n",
              " '1996',\n",
              " '1997',\n",
              " '1998',\n",
              " '1999',\n",
              " '19th',\n",
              " '19thc',\n",
              " '1and',\n",
              " '1h',\n",
              " '1h53',\n",
              " '1hr',\n",
              " '1st',\n",
              " '1ç',\n",
              " '20',\n",
              " '200',\n",
              " '2000',\n",
              " '2000s',\n",
              " '2001',\n",
              " '2002',\n",
              " '2003',\n",
              " '2004',\n",
              " '2005',\n",
              " '2006',\n",
              " '2007',\n",
              " '2008',\n",
              " '2009',\n",
              " '2010',\n",
              " '2015',\n",
              " '2017',\n",
              " '2022',\n",
              " '2036',\n",
              " '2047',\n",
              " '206',\n",
              " '2070',\n",
              " '2080',\n",
              " '20mins',\n",
              " '20mn',\n",
              " '20th',\n",
              " '21',\n",
              " '21st',\n",
              " '22',\n",
              " '223',\n",
              " '23',\n",
              " '230lbs',\n",
              " '237',\n",
              " '24',\n",
              " '242',\n",
              " '24th',\n",
              " '25',\n",
              " '250',\n",
              " '250000',\n",
              " '25mins',\n",
              " '25th',\n",
              " '26',\n",
              " '260',\n",
              " '26th',\n",
              " '27',\n",
              " '270',\n",
              " '2772',\n",
              " '28',\n",
              " '29',\n",
              " '2d',\n",
              " '2hrs',\n",
              " '2nd',\n",
              " '2x4',\n",
              " '30',\n",
              " '300',\n",
              " '3000',\n",
              " '300lb',\n",
              " '303',\n",
              " '30min',\n",
              " '30mins',\n",
              " '30s',\n",
              " '30th',\n",
              " '31',\n",
              " '31st',\n",
              " '32',\n",
              " '33',\n",
              " '330am',\n",
              " '34',\n",
              " '35',\n",
              " '36',\n",
              " '360',\n",
              " '36th',\n",
              " '37',\n",
              " '38',\n",
              " '39',\n",
              " '3bs',\n",
              " '3d',\n",
              " '3k',\n",
              " '3po',\n",
              " '3rd',\n",
              " '3rds',\n",
              " '3th',\n",
              " '3who',\n",
              " '40',\n",
              " '400',\n",
              " '40s',\n",
              " '40th',\n",
              " '41',\n",
              " '42',\n",
              " '42nd',\n",
              " '43',\n",
              " '44',\n",
              " '45',\n",
              " '46',\n",
              " '465',\n",
              " '47',\n",
              " '47s',\n",
              " '48',\n",
              " '49',\n",
              " '4eva',\n",
              " '4hrs',\n",
              " '4th',\n",
              " '4x',\n",
              " '50',\n",
              " '500',\n",
              " '5000',\n",
              " '50s',\n",
              " '51',\n",
              " '52',\n",
              " '53',\n",
              " '54',\n",
              " '540i',\n",
              " '55',\n",
              " '56',\n",
              " '57',\n",
              " '571',\n",
              " '58',\n",
              " '5hrs',\n",
              " '5min',\n",
              " '5th',\n",
              " '60',\n",
              " '600',\n",
              " '6000',\n",
              " '607',\n",
              " '60ies',\n",
              " '60s',\n",
              " '61',\n",
              " '62',\n",
              " '63',\n",
              " '637',\n",
              " '64',\n",
              " '65',\n",
              " '66',\n",
              " '666',\n",
              " '68',\n",
              " '69',\n",
              " '6th',\n",
              " '70',\n",
              " '700',\n",
              " '70ie',\n",
              " '70s',\n",
              " '71',\n",
              " '72',\n",
              " '73',\n",
              " '737',\n",
              " '74',\n",
              " '747',\n",
              " '75',\n",
              " '750',\n",
              " '76',\n",
              " '77',\n",
              " '78',\n",
              " '79',\n",
              " '7th',\n",
              " '80',\n",
              " '80ie',\n",
              " '80s',\n",
              " '82',\n",
              " '83',\n",
              " '84',\n",
              " '849',\n",
              " '85',\n",
              " '86',\n",
              " '87',\n",
              " '87minutes',\n",
              " '88',\n",
              " '89',\n",
              " '8p',\n",
              " '8star',\n",
              " '8th',\n",
              " '8½',\n",
              " '90',\n",
              " '900',\n",
              " '9000',\n",
              " '90210',\n",
              " '90s',\n",
              " '911',\n",
              " '92',\n",
              " '92nd',\n",
              " '93',\n",
              " '95',\n",
              " '96',\n",
              " '97',\n",
              " '974th',\n",
              " '98',\n",
              " '98minute',\n",
              " '99',\n",
              " '9999',\n",
              " '99cent',\n",
              " '99p',\n",
              " '9do',\n",
              " '9ers',\n",
              " '9s',\n",
              " '9th',\n",
              " '_x',\n",
              " 'a1',\n",
              " 'aa',\n",
              " 'aaargh',\n",
              " 'aaawwwwnnn',\n",
              " 'aag',\n",
              " 'aahhh',\n",
              " 'aap',\n",
              " 'aapke',\n",
              " 'aatish',\n",
              " 'aaww',\n",
              " 'ab',\n",
              " 'abahy',\n",
              " 'abandon',\n",
              " 'abandoned',\n",
              " 'abandonment',\n",
              " 'abank',\n",
              " 'abbas',\n",
              " 'abbasi',\n",
              " 'abbey',\n",
              " 'abbott',\n",
              " 'abbreviate',\n",
              " 'abbreviated',\n",
              " 'abby',\n",
              " 'abc',\n",
              " 'abdominal',\n",
              " 'abduct',\n",
              " 'abduction',\n",
              " 'abductor',\n",
              " 'abdul',\n",
              " 'abe',\n",
              " 'abel',\n",
              " 'aberration',\n",
              " 'abet',\n",
              " 'abetted',\n",
              " 'abhay',\n",
              " 'abhor',\n",
              " 'abhorrent',\n",
              " 'abide',\n",
              " 'abiding',\n",
              " 'abigail',\n",
              " 'ability',\n",
              " 'abilityof',\n",
              " 'abjectly',\n",
              " 'able',\n",
              " 'ably',\n",
              " 'abnormal',\n",
              " 'abnormally',\n",
              " 'abo',\n",
              " 'aboard',\n",
              " 'abode',\n",
              " 'abolition',\n",
              " 'abominable',\n",
              " 'abominably',\n",
              " 'abomination',\n",
              " 'aboriginal',\n",
              " 'aborigine',\n",
              " 'aborigines',\n",
              " 'aboriginies',\n",
              " 'aborigone',\n",
              " 'abort',\n",
              " 'abortion',\n",
              " 'abortionist',\n",
              " 'abos',\n",
              " 'abound',\n",
              " 'about',\n",
              " 'above',\n",
              " 'abraham',\n",
              " 'abrahamic',\n",
              " 'abrahams',\n",
              " 'abrams',\n",
              " 'abrasive',\n",
              " 'abre',\n",
              " 'abreast',\n",
              " 'abridge',\n",
              " 'abroad',\n",
              " 'abromowitz',\n",
              " 'abrupt',\n",
              " 'abruptly',\n",
              " 'abscond',\n",
              " 'absence',\n",
              " 'absent',\n",
              " 'absolute',\n",
              " 'absolutelly',\n",
              " 'absolutely',\n",
              " 'absolve',\n",
              " 'absorb',\n",
              " 'absorbing',\n",
              " 'abstain',\n",
              " 'abstract',\n",
              " 'abstraction',\n",
              " 'absurd',\n",
              " 'absurdism',\n",
              " 'absurdist',\n",
              " 'absurdity',\n",
              " 'absurdly',\n",
              " 'absurdness',\n",
              " 'abu',\n",
              " 'abunch',\n",
              " 'abundance',\n",
              " 'abundant',\n",
              " 'abundantly',\n",
              " 'abuse',\n",
              " 'abused',\n",
              " 'abuser',\n",
              " 'abusive',\n",
              " 'abut',\n",
              " 'abuzz',\n",
              " 'abysmal',\n",
              " 'abysmally',\n",
              " 'abyss',\n",
              " 'ac',\n",
              " 'academic',\n",
              " 'academy',\n",
              " 'acapulco',\n",
              " 'accede',\n",
              " 'accelerate',\n",
              " 'accent',\n",
              " 'accented',\n",
              " 'accentuate',\n",
              " 'accentuation',\n",
              " 'accept',\n",
              " 'acceptable',\n",
              " 'acceptance',\n",
              " 'acceptation',\n",
              " 'accepting',\n",
              " 'access',\n",
              " 'accessible',\n",
              " 'accessorize',\n",
              " 'accessory',\n",
              " 'accident',\n",
              " 'accidental',\n",
              " 'accidentally',\n",
              " 'accidentee',\n",
              " 'acclaim',\n",
              " 'acclaimed',\n",
              " 'acclimate',\n",
              " 'accolade',\n",
              " 'accommodate',\n",
              " 'accommodation',\n",
              " 'accompaniment',\n",
              " 'accompany',\n",
              " 'accompanying',\n",
              " 'accomplice',\n",
              " 'accomplish',\n",
              " 'accomplishe',\n",
              " 'accomplished',\n",
              " 'accomplishment',\n",
              " 'accord',\n",
              " 'accordian',\n",
              " 'according',\n",
              " 'accordingly',\n",
              " 'accost',\n",
              " 'account',\n",
              " 'accountant',\n",
              " 'accredit',\n",
              " 'accrue',\n",
              " 'accumulate',\n",
              " 'accuracy',\n",
              " 'accurate',\n",
              " 'accurately',\n",
              " 'accusation',\n",
              " 'accusations',\n",
              " 'accuse',\n",
              " 'accused',\n",
              " 'accustom',\n",
              " 'accustomed',\n",
              " 'ace',\n",
              " 'acedmy',\n",
              " 'acerbic',\n",
              " 'acharya',\n",
              " 'ache',\n",
              " 'achieve',\n",
              " 'achieved',\n",
              " 'achievement',\n",
              " 'achille',\n",
              " 'achillea',\n",
              " 'achilleas',\n",
              " 'achilles',\n",
              " 'achingly',\n",
              " 'acid',\n",
              " 'acidic',\n",
              " 'aciton',\n",
              " 'ackerman',\n",
              " 'ackland',\n",
              " 'acknowledge',\n",
              " 'acknowledgement',\n",
              " 'acmetropolis',\n",
              " 'acolyte',\n",
              " 'acoustic',\n",
              " 'acquaint',\n",
              " 'acquaintance',\n",
              " 'acquire',\n",
              " 'acquit',\n",
              " 'acquitted',\n",
              " 'acre',\n",
              " 'acres',\n",
              " 'acrobat',\n",
              " 'acrobatic',\n",
              " 'acropolis',\n",
              " 'act',\n",
              " 'actally',\n",
              " 'acte',\n",
              " 'acted',\n",
              " 'acting',\n",
              " 'actingjob',\n",
              " 'action',\n",
              " 'actioner',\n",
              " 'actioners',\n",
              " 'actions',\n",
              " 'activate',\n",
              " 'active',\n",
              " 'actively',\n",
              " 'activism',\n",
              " 'activist',\n",
              " 'activities',\n",
              " 'activity',\n",
              " 'actor',\n",
              " 'actors',\n",
              " 'actress',\n",
              " 'actresse',\n",
              " 'actresses',\n",
              " 'actual',\n",
              " 'actuality',\n",
              " 'actualize',\n",
              " 'actually',\n",
              " 'actualy',\n",
              " 'acuman',\n",
              " 'acupat',\n",
              " 'acute',\n",
              " 'acutely',\n",
              " 'ad',\n",
              " 'ada',\n",
              " 'adage',\n",
              " 'adam',\n",
              " 'adama',\n",
              " 'adamantly',\n",
              " 'adamos',\n",
              " 'adams',\n",
              " 'adamson',\n",
              " 'adapt',\n",
              " 'adaptation',\n",
              " 'adaptations',\n",
              " 'adapted',\n",
              " 'adapter',\n",
              " 'adaption',\n",
              " 'adaptor',\n",
              " 'add',\n",
              " 'addams',\n",
              " 'addendum',\n",
              " 'addict',\n",
              " 'addicted',\n",
              " 'addiction',\n",
              " 'addictive',\n",
              " 'adding',\n",
              " 'addio',\n",
              " 'addison',\n",
              " 'addition',\n",
              " 'additional',\n",
              " 'additionally',\n",
              " 'addle',\n",
              " 'addled',\n",
              " 'address',\n",
              " 'addy',\n",
              " 'adela',\n",
              " 'adelaide',\n",
              " 'ademir',\n",
              " 'adenoidal',\n",
              " 'adept',\n",
              " 'adeptly',\n",
              " 'adequate',\n",
              " 'adequately',\n",
              " 'adgth',\n",
              " 'adhd',\n",
              " 'adhere',\n",
              " 'adieu',\n",
              " 'adios',\n",
              " 'aditya',\n",
              " 'adj',\n",
              " 'adjacent',\n",
              " 'adjective',\n",
              " 'adjust',\n",
              " 'adjuster',\n",
              " 'adjustment',\n",
              " 'adjutant',\n",
              " 'adle',\n",
              " 'adler',\n",
              " 'adm',\n",
              " 'administration',\n",
              " 'administrator',\n",
              " 'admirable',\n",
              " 'admirably',\n",
              " 'admiral',\n",
              " 'admiralty',\n",
              " 'admiration',\n",
              " 'admire',\n",
              " 'admired',\n",
              " 'admirer',\n",
              " 'admires',\n",
              " 'admiring',\n",
              " 'admission',\n",
              " 'admit',\n",
              " 'admittadly',\n",
              " 'admittedly',\n",
              " 'adnan',\n",
              " 'ado',\n",
              " 'adobe',\n",
              " 'adolescence',\n",
              " 'adolescent',\n",
              " 'adolf',\n",
              " 'adolph',\n",
              " 'adopt',\n",
              " 'adoption',\n",
              " 'adoptive',\n",
              " 'adorable',\n",
              " 'adorably',\n",
              " 'adorble',\n",
              " 'adore',\n",
              " 'adored',\n",
              " 'adoree',\n",
              " 'adoringly',\n",
              " 'adorn',\n",
              " 'adr',\n",
              " 'adrenalin',\n",
              " 'adrenaline',\n",
              " 'adrian',\n",
              " 'adriana',\n",
              " 'adriatic',\n",
              " 'adrien',\n",
              " 'adrienne',\n",
              " 'adrift',\n",
              " 'adroit',\n",
              " 'adulhood',\n",
              " 'adult',\n",
              " 'adulterous',\n",
              " 'adultery',\n",
              " 'adulthood',\n",
              " 'adults',\n",
              " 'adv',\n",
              " 'advance',\n",
              " 'advanced',\n",
              " 'advancements',\n",
              " 'advances',\n",
              " 'advani',\n",
              " 'advantage',\n",
              " 'advent',\n",
              " 'adventure',\n",
              " 'adventurer',\n",
              " 'adventurers',\n",
              " 'adventures',\n",
              " 'adventuresome',\n",
              " 'adventurous',\n",
              " 'adversarial',\n",
              " 'adversary',\n",
              " 'adverse',\n",
              " 'adversely',\n",
              " 'advert',\n",
              " 'advertise',\n",
              " 'advertisement',\n",
              " 'advertising',\n",
              " 'advertize',\n",
              " 'advice',\n",
              " 'advise',\n",
              " 'advisedly',\n",
              " 'adviser',\n",
              " 'advisor',\n",
              " 'advocate',\n",
              " 'ae',\n",
              " 'aeon',\n",
              " 'aerial',\n",
              " 'aerobicide',\n",
              " 'aerodynamic',\n",
              " 'aeronautical',\n",
              " 'aeroplane',\n",
              " 'aesthetic',\n",
              " 'aesthetically',\n",
              " 'aestheticism',\n",
              " 'aetherial',\n",
              " 'afb',\n",
              " 'affable',\n",
              " 'affair',\n",
              " 'affaire',\n",
              " 'affairs',\n",
              " 'affect',\n",
              " 'affectation',\n",
              " 'affected',\n",
              " 'affectingly',\n",
              " 'affection',\n",
              " 'affectionate',\n",
              " 'affectionately',\n",
              " 'affiliate',\n",
              " 'affiliation',\n",
              " 'affinity',\n",
              " 'affirm',\n",
              " 'affirmation',\n",
              " 'affirmative',\n",
              " 'affirming',\n",
              " 'affleck',\n",
              " 'afflict',\n",
              " 'affluent',\n",
              " 'afford',\n",
              " 'affordable',\n",
              " 'affront',\n",
              " 'afgani',\n",
              " 'afghan',\n",
              " 'afghanistan',\n",
              " 'afi',\n",
              " 'aficionado',\n",
              " 'aflac',\n",
              " 'afleck',\n",
              " 'afloat',\n",
              " 'afm',\n",
              " 'afore',\n",
              " 'aforementione',\n",
              " 'aforementioned',\n",
              " 'afoul',\n",
              " 'afraid',\n",
              " 'africa',\n",
              " 'african',\n",
              " 'africans',\n",
              " 'afro',\n",
              " 'after',\n",
              " 'afterall',\n",
              " 'aftereffect',\n",
              " 'afterlife',\n",
              " 'aftermath',\n",
              " 'afternoon',\n",
              " 'afterstory',\n",
              " 'aftertaste',\n",
              " 'afterthought',\n",
              " 'afterward',\n",
              " 'afterwards',\n",
              " 'afterwhile',\n",
              " 'afterword',\n",
              " 'ag',\n",
              " 'again',\n",
              " 'against',\n",
              " 'agamemnon',\n",
              " 'agape',\n",
              " 'agatha',\n",
              " 'age',\n",
              " 'aged',\n",
              " 'agee',\n",
              " 'ageless',\n",
              " 'agency',\n",
              " 'agenda',\n",
              " 'agent',\n",
              " 'agents',\n",
              " 'ager',\n",
              " 'ages',\n",
              " 'aggie',\n",
              " 'aggravate',\n",
              " 'aggressed',\n",
              " 'aggression',\n",
              " 'aggressive',\n",
              " 'aggressively',\n",
              " 'aggressiveness',\n",
              " 'aggressor',\n",
              " 'aghast',\n",
              " 'agi',\n",
              " 'agile',\n",
              " 'agin',\n",
              " 'aging',\n",
              " 'agis',\n",
              " 'agitate',\n",
              " 'agitprop',\n",
              " 'agnes',\n",
              " 'agnostic',\n",
              " 'ago',\n",
              " 'agonise',\n",
              " 'agonisingly',\n",
              " 'agonize',\n",
              " 'agonized',\n",
              " 'agonizing',\n",
              " 'agonizingly',\n",
              " 'agony',\n",
              " 'agrawal',\n",
              " 'agree',\n",
              " 'agreeable',\n",
              " 'agreement',\n",
              " 'agricultural',\n",
              " 'agriculture',\n",
              " 'aguirre',\n",
              " 'agy',\n",
              " 'ah',\n",
              " 'aha',\n",
              " 'ahab',\n",
              " 'ahah',\n",
              " 'ahead',\n",
              " 'ahem',\n",
              " 'ahet',\n",
              " 'ahh',\n",
              " 'ahhh',\n",
              " 'ahhhh',\n",
              " 'ahlberg',\n",
              " 'ahmad',\n",
              " 'ahmed',\n",
              " 'ai',\n",
              " 'aicha',\n",
              " 'aid',\n",
              " 'aida',\n",
              " 'aidan',\n",
              " 'aide',\n",
              " 'aiden',\n",
              " 'aids',\n",
              " 'aiello',\n",
              " 'aikido',\n",
              " 'ail',\n",
              " 'aileen',\n",
              " 'ailing',\n",
              " 'ailton',\n",
              " 'aim',\n",
              " 'aime',\n",
              " 'aimee',\n",
              " 'aiming',\n",
              " 'aimless',\n",
              " 'aimlessly',\n",
              " 'aimée',\n",
              " 'aip',\n",
              " 'air',\n",
              " 'airbag',\n",
              " 'airball',\n",
              " 'airborne',\n",
              " 'aircraft',\n",
              " 'airdate',\n",
              " 'airhead',\n",
              " 'airial',\n",
              " 'airing',\n",
              " 'airline',\n",
              " 'airliner',\n",
              " 'airman',\n",
              " 'airplane',\n",
              " 'airplay',\n",
              " 'airport',\n",
              " 'airship',\n",
              " 'airspace',\n",
              " 'airtight',\n",
              " 'airwave',\n",
              " 'airway',\n",
              " 'airwolf',\n",
              " 'aishwarya',\n",
              " 'aisle',\n",
              " 'aislinn',\n",
              " 'aja',\n",
              " 'ajay',\n",
              " 'ajeeb',\n",
              " 'ajnabe',\n",
              " 'ak',\n",
              " 'aka',\n",
              " 'akin',\n",
              " 'akira',\n",
              " 'akosua',\n",
              " 'akroyd',\n",
              " 'akshay',\n",
              " 'akshaye',\n",
              " 'akshey',\n",
              " 'akuzi',\n",
              " 'al',\n",
              " 'ala',\n",
              " 'alabama',\n",
              " 'aladdin',\n",
              " 'alahani',\n",
              " 'alain',\n",
              " 'alamo',\n",
              " 'alan',\n",
              " 'alannis',\n",
              " 'alarm',\n",
              " 'alarmed',\n",
              " 'alarming',\n",
              " 'alas',\n",
              " 'alaska',\n",
              " 'alaskey',\n",
              " 'alba',\n",
              " 'albany',\n",
              " 'albatross',\n",
              " 'albeit',\n",
              " 'albeniz',\n",
              " 'alberni',\n",
              " 'albert',\n",
              " 'alberta',\n",
              " ...]"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv.get_feature_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGFAtTsknl2m"
      },
      "source": [
        "Vamos a encontrar muchisimas palabras que no tienen sentido y no aportan nada a nuestro modelo, o caracteres como \"__________________________________________________________________\".\n",
        "Todo esto podríamos tenerlo en cuenta para la etapa de preprocesamiento.\n",
        "\n",
        "Con el X_train y X_test que generamos con nuestro countVectorizer ya podríamos entrenar un modelo.\n",
        "\n",
        "Ahora, aplicaremos TF IDF\n",
        "\n",
        "\n",
        "### TFIDF\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "\n",
        "Generar X_train, X_test, y_train e y_test de nuevo, ya que las modificamos anteriormente con el count vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inqnSYJyoFF0"
      },
      "outputs": [],
      "source": [
        "X = df.REVIEW.copy()\n",
        "y = df.TARGET.copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmY2Lc-1oHuW"
      },
      "source": [
        "Ahora, importar tfidf vectorizer y aplicarlo sobre X_train y X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqND7dh4oFI9"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqhOwXu7oQMe"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svdA2OkloXqD",
        "outputId": "7dfe701b-c14f-4392-b0be-8dceafeff88f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<4000x29434 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 342225 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MvwGOBroYpj",
        "outputId": "26d6cb73-5ad9-4303-86d2-f7e9ce657ea3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1000x29434 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 81583 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 177,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEqLJ5yLoac0"
      },
      "source": [
        "Entrenar un SVC con los datos ya vectorizados.\n",
        "\n",
        "Utilizaremos: \n",
        "- random_state=0\n",
        "- C=0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mje1CSJpogxN",
        "outputId": "50ee11fc-6e95-425a-d9ea-8a1ebb3e2658"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SVC(C=0.5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=0, shrinking=True, tol=0.001,\n",
              "    verbose=False)"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "clf = SVC(random_state=0, C=0.5)\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivIslAjUEOMm"
      },
      "source": [
        "Ahora, como siempre hicimos, podemos medir métricas. Por ejemplo, imprimir el classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCloPJhYoyaQ",
        "outputId": "53cb07f6-f412-4c09-ec72-45dcf4d2f915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NEG       0.98      0.96      0.97      1981\n",
            "         POS       0.96      0.99      0.97      2019\n",
            "\n",
            "    accuracy                           0.97      4000\n",
            "   macro avg       0.97      0.97      0.97      4000\n",
            "weighted avg       0.97      0.97      0.97      4000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NEG       0.90      0.75      0.82       519\n",
            "         POS       0.77      0.91      0.84       481\n",
            "\n",
            "    accuracy                           0.83      1000\n",
            "   macro avg       0.84      0.83      0.83      1000\n",
            "weighted avg       0.84      0.83      0.83      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_train, clf.predict(X_train)))\n",
        "print(classification_report(y_test, clf.predict(X_test)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_intro_resuelto.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "892d461b55a6ce994a56bafd67ae2f3489d9f23234c096cfb51dfe498c166e4b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
