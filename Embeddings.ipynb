{"cells":[{"cell_type":"markdown","metadata":{"id":"xm2RXim5QijG"},"source":["# Modelos de spacy\n","\n","Como vimos en la teoría, en general no vamos a entrenar embeddings desde 0 sino que utilizaremos embeddings ya entrenados y luego, si es necesario, los adaptaremos a nuestros datos (no en esta clase).\n","\n","En este notebook, queremos acceder a embeddings de palabras. El modelo de spacy con el que venimos trabajando hasta ahora es: [**en_core_web_sm**](https://spacy.io/models/en#en_core_web_sm) (35MB). El mismo nos provee vocabulario, sintaxis, entidades y todo lo que estuvimos viendo hasta ahora, pero no embeddings.\n","\n","Para utilizar embeddings (en inglés) en spacy tenemos las siguientes opciones: \n","> [**en_core_web_md**](https://spacy.io/models/en#en_core_web_md) (116MB) Vectors: 685k keys, 20k unique vectors (300 dimensions)\n","\n","> [**en_core_web_lg**](https://spacy.io/models/en#en_core_web_lg) (812MB) Vectors: 685k keys, 685k unique vectors (300 dimensions)\n","\n","> [**en_vectors_web_lg**](https://spacy.io/models/en#en_vectors_web_lg) (631MB) Vectors: 1.1m keys, 1.1m unique vectors (300 dimensions)\n","\n","En nuestro caso, con  **en_core_web_md** será suficiente. Pueden probar luego los otros modelos.\n","\n","Por defecto en colab no tenemos los modelos más pesados, por lo que **la siguiente celda les dará un error:**"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python -m spacy download en_core_web_md"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"elapsed":317,"status":"error","timestamp":1635884219308,"user":{"displayName":"Federico Baiocco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf2wyATB48QBNVeVnR-1Y2rw_O5uZLAvphY2Otz9c=s64","userId":"14260194928165318342"},"user_tz":180},"id":"Le8uzebpRzlt","outputId":"6c3295c3-be83-43cb-8e16-ccd67f3dd2ed"},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en_core_web_md')"]},{"cell_type":"markdown","metadata":{"id":"Fi46Fih_R2DJ"},"source":["Por lo tanto debemos descargarlo:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29756,"status":"ok","timestamp":1635884188519,"user":{"displayName":"Federico Baiocco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf2wyATB48QBNVeVnR-1Y2rw_O5uZLAvphY2Otz9c=s64","userId":"14260194928165318342"},"user_tz":180},"id":"pr5Jg1xtSy6B","outputId":"38bb0f26-d9b7-4bb0-e2fb-1056b30dd58b"},"outputs":[],"source":["# !python -m spacy download en_core_web_lg"]},{"cell_type":"markdown","metadata":{"id":"yoHeDop_S-9R"},"source":["Luego de instalarlo deberán resetear su runtime de colab.\n","\n","Para esto van a runtime -> restart runtime.\n","\n","Ahora si deberían poder cargar el modelo:"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":22286,"status":"ok","timestamp":1635884326011,"user":{"displayName":"Federico Baiocco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf2wyATB48QBNVeVnR-1Y2rw_O5uZLAvphY2Otz9c=s64","userId":"14260194928165318342"},"user_tz":180},"id":"2WFvXaMVTAzK"},"outputs":[],"source":["# import spacy\n","# nlp = spacy.load('en_core_web_md')"]},{"cell_type":"markdown","metadata":{"id":"5M-YYuomUTDx"},"source":["# Embeddings en spacy\n","\n","En spacy vimos que los vectores que se utilizan tienen 300 dimensiones.\n","\n","Podemos acceder a los mismos de la siguiente manera:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aoQY8cGWQijJ","outputId":"1f476f59-38d4-4bba-8113-a530b4dc4d07","scrolled":true},"outputs":[],"source":["nlp('lion').vector"]},{"cell_type":"markdown","metadata":{"id":"cszIC30-QijK"},"source":["Si le pedimos el embedding de un documento, lo calculara como el promedio de los vectores de todas las palabras. Con esto, podemos computar similaridad entre documentos."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1635885209818,"user":{"displayName":"Federico Baiocco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf2wyATB48QBNVeVnR-1Y2rw_O5uZLAvphY2Otz9c=s64","userId":"14260194928165318342"},"user_tz":180},"id":"EgzwPE3zQijK","outputId":"6407c806-f10c-4eeb-f5a1-9528914e5613"},"outputs":[],"source":["doc = nlp(u'The quick brown fox jumped over the lazy dogs.')\n","\n","doc.vector"]},{"cell_type":"markdown","metadata":{"id":"MSkWR18EQijL"},"source":["# Identificando vectores similares\n","\n","Spacy nos provee el método .similarity() para evaluar la similitud entre palabras.\n","\n","El método similarity es de los tokens."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":324,"status":"ok","timestamp":1635885326722,"user":{"displayName":"Federico Baiocco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf2wyATB48QBNVeVnR-1Y2rw_O5uZLAvphY2Otz9c=s64","userId":"14260194928165318342"},"user_tz":180},"id":"nS-cVUVHQijL","outputId":"3d0547aa-d4d0-4f1f-9333-486f4685494d","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["lion lion 1.0\n","lion cat 0.3854507803916931\n","lion pet 0.20031584799289703\n","cat lion 0.3854507803916931\n","cat cat 1.0\n","cat pet 0.732966423034668\n","pet lion 0.20031584799289703\n","pet cat 0.732966423034668\n","pet pet 1.0\n"]}],"source":["tokens = nlp('lion cat pet')\n","\n","for token1 in tokens:\n","    for token2 in tokens:\n","        print(token1.text, token2.text, token1.similarity(token2))"]},{"cell_type":"markdown","metadata":{"id":"WBGP7uOOQijM"},"source":["<font color=red>El orden NO importa. `token1.similarity(token2)` retorna lo mismo que `token2.similarity(token1)`.</font>"]},{"cell_type":"markdown","metadata":{"id":"A28aqjc5QijM"},"source":["Como es de esperar, vemos la relación más fuerte entre \"cat\" y \"pet\" y la más débil es \"lion\" con \"pet\"."]},{"cell_type":"markdown","metadata":{"id":"QxrQTF1iQijN"},"source":["# Palabras opuestas\n","\n","Como palabras opuestas pueden aparecer muchas veces en los mismos contextos, seguramente vamos a encontrar casos con palabras opuestas y distancias pequeñas."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":322,"status":"ok","timestamp":1635885463822,"user":{"displayName":"Federico Baiocco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf2wyATB48QBNVeVnR-1Y2rw_O5uZLAvphY2Otz9c=s64","userId":"14260194928165318342"},"user_tz":180},"id":"DxGc8V0pQijN","outputId":"19a1525f-0067-4fca-dc30-e184104c3ad0"},"outputs":[{"name":"stdout","output_type":"stream","text":["like like 1.0\n","like love 0.5212638974189758\n","like hate 0.5065141320228577\n","love like 0.5212638974189758\n","love love 1.0\n","love hate 0.5708349943161011\n","hate like 0.5065141320228577\n","hate love 0.5708349943161011\n","hate hate 1.0\n"]}],"source":["tokens = nlp('like love hate')\n","\n","for token1 in tokens:\n","    for token2 in tokens:\n","        print(token1.text, token2.text, token1.similarity(token2))"]},{"cell_type":"markdown","metadata":{"id":"aicBfncPQijO"},"source":["# Operaciones con vectores\n","\n","También podemos calcular nuevos vectores haciendo sumas y restas. Existe un ejemplo famoso sobre Word2vec que dice que:\n","\n","<pre>\"king\" - \"man\" + \"woman\" = \"queen\"</pre>\n","\n","Lo podemos probar:"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22366,"status":"ok","timestamp":1635886977343,"user":{"displayName":"Federico Baiocco","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf2wyATB48QBNVeVnR-1Y2rw_O5uZLAvphY2Otz9c=s64","userId":"14260194928165318342"},"user_tz":180},"id":"dmRX83qyQijO","outputId":"659bd8ee-68d1-4b3d-9182-aee17e0c9ac4"},"outputs":[{"name":"stdout","output_type":"stream","text":["['queen:0.6178014278411865', 'havin:0.3667823076248169', 'where:0.3385923206806183', 'woman:0.30994713306427', 'somethin:0.30953145027160645', 'there:0.30542072653770447', 'should:0.29837310314178467', 'these:0.29441288113594055', 'would:0.2928425371646881', 'nothin:0.2927696406841278']\n"]}],"source":["from scipy import spatial\n","\n","#Función para calcular distancia coseno\n","def cosine_similarity(x, y):\n","  return 1 - spatial.distance.cosine(x, y) \n","\n","king = nlp.vocab['king'].vector\n","man = nlp.vocab['man'].vector\n","woman = nlp.vocab['woman'].vector\n","\n","new_vector = king - man + woman\n","computed_similarities = []\n","\n","# Comparamos con todo el vocabulario\n","for word in nlp.vocab:\n","    # Ignoramos palabras sin embedding en el modelo\n","    if word.has_vector:\n","      #Ignoramos palabras en mayúsculas\n","      if word.is_lower:\n","            # Nos quedamos con palabras\n","            if word.is_alpha:\n","                if len(word.text)>4:\n","              #Calculamos distancia coseno\n","                  similarity = cosine_similarity(new_vector, word.vector)\n","                  #similarity = norm(new_vector-word.vector)\n","              # Se almacenan los resultados en la lista de tuplas\n","              # Cada tupla tiene como primer valor el token y segundo valor la similaridad\n","                  computed_similarities.append((word, similarity))\n","\n","# Ordenamos de mayor a menor\n","computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n","\n","print([f\"{w[0].text}:{w[1]}\" for w in computed_similarities[:10]])"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['wife:0.04696419835090637', 'queen:0.03473014757037163', 'these:0.000905352586414665', 'space:-0.011037594638764858', 'were:-0.014306643046438694', 'should:-0.023915868252515793', 'does:-0.029595397412776947', 'where:-0.0332501120865345', 'need:-0.03333720564842224', 'would:-0.04728491231799126']\n"]}],"source":["king = nlp.vocab['husband '].vector\n","man = nlp.vocab['man'].vector\n","woman = nlp.vocab['woman'].vector\n","nlp.vocab['wife'].vector\n","\n","new_vector = king - man + woman\n","computed_similarities = []\n","\n","# Comparamos con todo el vocabulario\n","for word in nlp.vocab:\n","    # Ignoramos palabras sin embedding en el modelo\n","    if word.has_vector:\n","      #Ignoramos palabras en mayúsculas\n","      if word.is_lower:\n","            # Nos quedamos con palabras\n","            if word.is_alpha:\n","                if len(word.text)>3:\n","              # Calcular la similitud con cada palabra\n","                    similarity =np.dot(new_vector, word.vector) / (np.linalg.norm(new_vector) * np.linalg.norm(word.vector))\n","                    computed_similarities.append((word, similarity))\n","\n","# Ordenar las similitudes por mayor similitud\n","sorted_similarities = sorted(computed_similarities, key=lambda x: x[1], reverse=True)\n","\n","print([f\"{w[0].text}:{w[1]}\" for w in sorted_similarities[:10]])"]},{"cell_type":"markdown","metadata":{"id":"DD4Pyi_KQijP"},"source":["En este caso, con estos embeddings no nos dió que la palabra más similar es \"queen\", pero sale en el 2do lugar!"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["queen = nlp.vocab['queen'].vector"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["dog = nlp.vocab['dog']"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"text/plain":["'dog'"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["dog.text"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["from numpy.linalg import norm"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["6.82974"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["norm(queen)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"data":{"text/plain":["7.9464583"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["norm(new_vector)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":["4.9243603"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["norm(queen-new_vector)"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Embeddings.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"vscode":{"interpreter":{"hash":"892d461b55a6ce994a56bafd67ae2f3489d9f23234c096cfb51dfe498c166e4b"}}},"nbformat":4,"nbformat_minor":0}
