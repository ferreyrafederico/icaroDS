{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f241a42",
   "metadata": {},
   "source": [
    "# Descargar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c26615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descargar siempre cuando usen Colab\n",
    "#!pip install praw tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea947862",
   "metadata": {},
   "source": [
    "# Iniciar el Extractor y Completar debajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4d9ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recopilando publicaciones: 977it [02:07,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han extraído y guardado exitosamente 8661 publicaciones del subreddit 'argentina'.\n",
      "Proceso finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configura tu instancia de PRAW con tus credenciales de la aplicación y un user_agent único\n",
    "reddit_config = {\n",
    "    'client_id': '9zK4SSfBSCMJPnpX1rbg4Q',\n",
    "    'client_secret': 'CI5_-mXS5NGVZMvNjmOb6kyE60fG_A',\n",
    "    'user_agent': 'MiScriptRedditBot/v1.7 (by Sheincito en Reddit)'\n",
    "}\n",
    "\n",
    "# Define el subreddit a extraer\n",
    "subreddit_to_extract = input(\"Ingrese el nombre del subreddit: \")\n",
    "\n",
    "# Define el filtro para las publicaciones\n",
    "filtro = input(\"Ingrese el filtro para las publicaciones (deje en blanco para no filtrar): \")\n",
    "\n",
    "# Nombre del archivo CSV basado en el subreddit extraído\n",
    "csv_filename = f\"extractor_{subreddit_to_extract}.csv\"\n",
    "\n",
    "# Función para cargar los títulos existentes del archivo CSV\n",
    "def cargar_titulos_existentes(filename, subreddit_name):\n",
    "    if os.path.isfile(filename):  \n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "            if df.empty:\n",
    "                return df\n",
    "            else:\n",
    "                return set(df['Reddit'])\n",
    "        except pd.errors.EmptyDataError:\n",
    "            return df\n",
    "    else: \n",
    "        return pd.DataFrame(columns=[subreddit_name])\n",
    "\n",
    "# Función para recopilar publicaciones\n",
    "def recopilar_publicaciones(subreddit_name, filename, reddit, filtro=None):\n",
    "    # Cargar los títulos existentes del archivo CSV\n",
    "    titulos_existentes = cargar_titulos_existentes(filename, subreddit_name)\n",
    "    \n",
    "    # Obtiene el subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # Lista para almacenar las publicaciones\n",
    "    posts = []\n",
    "    \n",
    "    # Convertir el filtro a minúsculas si está presente\n",
    "    filtro = filtro.lower() if filtro else None\n",
    "    \n",
    "    # Dividir el filtro en palabras si contiene varias palabras\n",
    "    filtro_palabras = filtro.split() if filtro else None\n",
    "    \n",
    "    # Bucle para recopilar publicaciones\n",
    "    for submission in tqdm(subreddit.new(limit=None), desc='Recopilando publicaciones'):\n",
    "        # Convertir el título a minúsculas para comparación sin distinción de mayúsculas y minúsculas\n",
    "        titulo_lower = submission.title.lower()\n",
    "        \n",
    "        # Aplicar filtro si está presente\n",
    "        if filtro:\n",
    "            # Verificar si alguna de las palabras del filtro está en el título de la publicación\n",
    "            if not any(word in titulo_lower for word in filtro_palabras):\n",
    "                continue\n",
    "        \n",
    "        # Verifica si el título de la publicación ya está en los títulos existentes\n",
    "        if submission.title not in titulos_existentes:\n",
    "            # Agrega el título a la lista de títulos y al conjunto de títulos existentes\n",
    "            titulos_existentes.add(submission.title)\n",
    "            \n",
    "            # Agrega el título y el cuerpo de la publicación a la lista de publicaciones\n",
    "            posts.append({'Reddit': submission.title, 'Body': submission.selftext})\n",
    "    \n",
    "            # Convierte la lista de publicaciones en un DataFrame\n",
    "            df = pd.DataFrame(posts)\n",
    "    \n",
    "            # Guarda las publicaciones en un archivo CSV (modo 'a' para agregar datos sin sobrescribir)\n",
    "            df.to_csv(filename, index=False, mode='a', header=not os.path.exists(filename))\n",
    "            \n",
    "            # Espera 1 segundo antes de procesar la siguiente publicación\n",
    "            time.sleep(1)\n",
    "\n",
    "# Configura PRAW con la configuración especificada\n",
    "reddit = praw.Reddit(**reddit_config)\n",
    "\n",
    "# Llama a la función para recopilar publicaciones y guardarlas en un archivo CSV\n",
    "recopilar_publicaciones(subreddit_to_extract, csv_filename, reddit, filtro)\n",
    "\n",
    "# Cuenta las líneas del archivo CSV para determinar cuántas publicaciones se extrajeron\n",
    "with open(csv_filename, 'r', encoding='utf-8') as file:\n",
    "    total_publicaciones = sum(1 for line in file)\n",
    "\n",
    "# Mensaje de finalización\n",
    "print(f\"Se han extraído y guardado exitosamente {total_publicaciones} publicaciones del subreddit '{subreddit_to_extract}'.\")\n",
    "print(\"Proceso finalizado.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9315eb18",
   "metadata": {},
   "source": [
    "# Cargamos el DF generado para probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368b06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el archivo CSV que contiene los datos del DataFrame\n",
    "df = pd.read_csv(csv_filename)\n",
    "pd.options.display.max_colwidth = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508e449",
   "metadata": {},
   "source": [
    "## Aqui borre los nulos por que cuando hay imagenes no las guarda y queda como NaN en el Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc622327",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.dropna(subset=['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3bafbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c30c582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2['Reddit'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
